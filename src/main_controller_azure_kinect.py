"""
Virtra ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ ëª¨ë“ˆ (Azure Kinect í†µí•© ë²„ì „)
ê¸°ì¡´ ëª¨ë“ˆí™” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ Azure Kinect DK ì§€ì› ì¶”ê°€

í•µì‹¬ ë³€ê²½ì‚¬í•­:
- camera_manager.pyë¥¼ ì‚¬ìš©í•œ í†µí•© ì¹´ë©”ë¼ ê´€ë¦¬
- Azure Kinect DK (ë‹¨ì¼/ë“€ì–¼) ì§€ì› ì¶”ê°€
- ê¸°ì¡´ laser_detector_core, frame_processing, screen_mapping ëª¨ë“ˆ ê·¸ëŒ€ë¡œ ìœ ì§€
- ê¹Šì´ ì •ë³´ í™œìš© ê°•í™”
"""

import cv2
import numpy as np
import time
import logging
import json
import socket
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

# ê¸°ì¡´ ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€)
from laser_detector_core import LaserDetectorCore, LaserDetectionResult
from frame_processing import FrameProcessor
from screen_mapping import ScreenMapper
from gun_muzzle_detector import MuzzleDetector
from body_tracking_worker import BodyTrackingWorker

# Enhanced Laser Detector (ì„ íƒì  ì‚¬ìš©)
try:
    from enhanced_laser_detector import EnhancedLaserDetector
    ENHANCED_DETECTOR_AVAILABLE = True
    print("[INFO] Enhanced Laser Detector (Modified CHT) Available")
except ImportError:
    ENHANCED_DETECTOR_AVAILABLE = False
    print("[INFO] Enhanced Laser Detector NOT Available - Using Standard Mode")

# ìƒˆë¡œìš´ ì¹´ë©”ë¼ ê´€ë¦¬ ëª¨ë“ˆ
from camera_manager import (
    CameraManager, CameraType, CameraConfig, CameraFrame,
    create_webcam_manager, create_zed_manager, 
    create_azure_kinect_manager, create_dual_azure_kinect_manager
)


class MainControllerAzureKinect:
    """
    Azure Kinect í†µí•© ë©”ì¸ ì‹œìŠ¤í…œ ì»¨íŠ¸ë¡¤ëŸ¬
    
    ë‹´ë‹¹ ê¸°ëŠ¥:
    - ë‹¤ì–‘í•œ ì¹´ë©”ë¼ íƒ€ì… ì§€ì› (ì›¹ìº , ZED, Azure Kinect)
    - ê¸°ì¡´ ëª¨ë“ˆí™” êµ¬ì¡° ìœ ì§€
    - ê¹Šì´ ì •ë³´ í™œìš© ê°•í™”
    - ë“€ì–¼ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ ì§€ì›
    """
    
    def __init__(self, camera_type: str = "azure_kinect", **camera_kwargs):
        """ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.camera_type = camera_type.lower()
        self.camera_kwargs = camera_kwargs
        
        # ì¹´ë©”ë¼ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.camera_manager = self._create_camera_manager()
        self.secondary_camera_manager = None  # ë“€ì–¼ ëª¨ë“œìš©
        
        # ê¸°ì¡´ ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ë“¤ (Azure Kinect ìµœì í™”)
        self.laser_core = LaserDetectorCore()
        # ë“€ì–¼ ëª¨ë“œ ë¶„ë¦¬: ì¹´ë©”ë¼ë³„ ì½”ì–´ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒíƒœ ê°„ì„­(ì´ì „ í”„ë ˆì„, ëª¨ì…˜ íˆìŠ¤í† ë¦¬, ROI)ì„ ì œê±°
        self.laser_core_screen = LaserDetectorCore()
        self.laser_core_gun = LaserDetectorCore()
        # ë“€ì–¼ ëª¨ë“œ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì¹´ë©”ë¼ë³„ í”„ë ˆì„ í”„ë¡œì„¸ì„œ ë¶„ë¦¬
        self.frame_processor = FrameProcessor()
        self.frame_processor.enable_frame_diff = True  # í•µì‹¬: í”„ë ˆì„ ì°¨ì´ ê²€ì¶œ í™œì„±í™” (ì†ì„± ì„¤ì •)
        self.frame_processor_primary = FrameProcessor()
        self.frame_processor_primary.enable_frame_diff = True
        self.frame_processor_secondary = FrameProcessor()
        self.frame_processor_secondary.enable_frame_diff = True
        self.screen_mapper = ScreenMapper()
        # ë³´ì¡° ì¹´ë©”ë¼ ì´êµ¬ ì „ìš© ê²€ì¶œê¸°
        self.muzzle_detector = MuzzleDetector()
        
        # Body Tracking ì›Œì»¤ (ì´êµ¬ ROI ë™ì  ì—…ë°ì´íŠ¸ìš©)
        self.bt_worker = None
        self.enable_bt = bool(camera_kwargs.get('enable_bt', False))
        
        # Enhanced Detector (ì„ íƒì  ì‚¬ìš©) - ê¸°ë³¸ì€ ë¹„í™œì„±í™”
        self.use_enhanced_detector = camera_kwargs.get('use_enhanced_detector', False)
        self.enhanced_laser_core = None
        
        if ENHANCED_DETECTOR_AVAILABLE and self.use_enhanced_detector:
            # Enhanced Detector ì‚¬ìš© (ê¸°ë³¸ì€ CHT ë¹„í™œì„±í™” ìƒíƒœ)
            enable_cht = camera_kwargs.get('enable_cht', False)
            self.enhanced_laser_core = EnhancedLaserDetector(enable_cht=enable_cht)
            print(f"[INFO] Enhanced Detector Active (CHT: {'ON' if enable_cht else 'OFF'})")
        else:
            print("[INFO] Standard Laser Detector Active")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.is_running = False
        self.current_frame = None
        self.current_depth = None
        self.last_clicked_rgbd = None  # Wí‚¤ ìƒ˜í”Œë§ìš© ì €ì¥ì†Œ
        
        # ë“€ì–¼ ì¹´ë©”ë¼ìš© ë³´ì¡° í”„ë ˆì„
        self.secondary_frame = None
        self.secondary_depth = None
        self._last_secondary_result = None  # ë³´ì¡° ì¹´ë©”ë¼ ê²€ì¶œ ê²°ê³¼ ì €ì¥ìš©
        
        self.detection_results = []
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'frames_processed': 0,
            'detections': 0,
            'depth_detections': 0,
            'triangulated_points': 0,
            'start_time': time.time(),
            'fps': 0.0
        }
        
        # UI ì„¤ì •
        self.show_debug = False
        self.show_depth = True  # ê¹Šì´ ë§µ ê¸°ë³¸ í‘œì‹œ
        self.show_calibration = False  # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ
        
        # RGB ë””ë²„ê¹…ìš© ë³€ìˆ˜ ì¶”ê°€
        self.current_image = None  # í˜„ì¬ BGR ì´ë¯¸ì§€
        self.current_gray = None   # í˜„ì¬ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
        
        # ë¡œê¹…
        self.logger = logging.getLogger(__name__)
        
        print(f"[INFO] Azure Kinect Main Controller Initialized")
        print(f"[INFO] Camera Type: {self.camera_type}")
        
        # Azure Kinect ì „ìš© ì„¤ì •
        self.azure_kinect_config = {
            'use_depth_filtering': True,
            'depth_range_mm': (500, 3000),
            'triangulation_enabled': camera_type == "dual_azure_kinect",
            'subpixel_accuracy': True,
            
            # 3D ë²¡í„° ìƒì„±ì„ ìœ„í•œ RGB í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • (ë ˆì´ì € ë°ê¸° ê¸°ë°˜)
            'primary_brightness_threshold': 120,   # Camera #1: ìŠ¤í¬ë¦° ë ˆì´ì € í¬ì¸íŠ¸ ë°ê¸°
            'secondary_brightness_threshold': 120  # Camera #2: ë ˆì´ì € í¬ì¸í„° ì´êµ¬ ë°ê¸°
        }

        # ìŠ¤í¬ë¦° í‰ë©´(ì›”ë“œ) ì •ì˜: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ì„ ìš°ì„  ì‚¬ìš©
        self.screen_plane = self._load_screen_plane_from_calibration()
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì„¤ì • (ì—í”¼í´ë¼ì¸ ì§€ì˜¤ë©”íŠ¸ë¦¬ìš©)
        self.calibration_config = {
            'chessboard_size': (8, 6),  # ì²´ìŠ¤ë³´ë“œ ë‚´ë¶€ ì½”ë„ˆ ê°œìˆ˜ (ê°€ë¡œ, ì„¸ë¡œ) - ì‚¬ìš©ì ì´ë¯¸ì§€ ê¸°ì¤€
            'square_size': 80.0,  # ì²´ìŠ¤ë³´ë“œ ì •ì‚¬ê°í˜• í¬ê¸° (mm) - A1 ì‚¬ì´ì¦ˆ ê¸°ì¤€
            'min_corners_detected': 20,  # ìµœì†Œ ê²€ì¶œ ì½”ë„ˆ ìˆ˜
            'subpix_criteria': (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
        }
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.calibration_only_mode = False  # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì „ìš© ëª¨ë“œ (ë ˆì´ì € ê²€ì¶œ ë¹„í™œì„±í™”)
        self.chessboard_detection_interval = 2  # ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ ê°„ê²© (ë§¤ Ní”„ë ˆì„ë§ˆë‹¤) - ì‹¤ì‹œê°„ í‘œì‹œ
        self.chessboard_frame_counter = 0  # ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ í”„ë ˆì„ ì¹´ìš´í„°
        
        # ë“€ì–¼ ì¹´ë©”ë¼ ì„±ëŠ¥ ìµœì í™”
        self.frame_skip_counter = 0  # í”„ë ˆì„ ìŠ¤í‚µ ì¹´ìš´í„°
        self.frame_skip_interval = 1  # ë§¤ Ní”„ë ˆì„ë§ˆë‹¤ë§Œ ì²˜ë¦¬ (1=ëª¨ë“  í”„ë ˆì„, 2=ê²©í”„ë ˆì„)
        self.last_frame_time = time.time()  # FPS ì¸¡ì •ìš©
        
        # ë“€ì–¼ ëª¨ë“œ ìµœì í™”: ROI ìë™ ì„¤ì •
        if camera_type == "dual_azure_kinect":
            self._setup_dual_mode_roi()
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì €ì¥ìš©
        self.calibration_data = {
            'primary_corners': [],      # 1ë²ˆ ì¹´ë©”ë¼ ì½”ë„ˆ ì¢Œí‘œë“¤ (ê° ì´ë¯¸ì§€ë³„)
            'secondary_corners': [],    # 2ë²ˆ ì¹´ë©”ë¼ ì½”ë„ˆ ì¢Œí‘œë“¤ (ê° ì´ë¯¸ì§€ë³„)
            'object_points': [],        # 3D ì›”ë“œ ì¢Œí‘œ (ê° ì´ë¯¸ì§€ë³„)
            'primary_images': [],       # 1ë²ˆ ì¹´ë©”ë¼ ì›ë³¸ ì´ë¯¸ì§€ë“¤
            'secondary_images': [],     # 2ë²ˆ ì¹´ë©”ë¼ ì›ë³¸ ì´ë¯¸ì§€ë“¤
            'image_count': 0,
            'calibration_complete': False,
            'min_images_required': 20,  # ìµœì†Œ í•„ìš” ì´ë¯¸ì§€ ìˆ˜ (í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ì¦ê°€)
            'max_images_allowed': 30,   # ìµœëŒ€ ìˆ˜ì§‘ ì´ë¯¸ì§€ ìˆ˜
            'primary_detected': False,
            'secondary_detected': False,
            'last_capture_time': 0      # ë§ˆì§€ë§‰ ìº¡ì²˜ ì‹œê°„ (ì¤‘ë³µ ë°©ì§€)
        }
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ ì €ì¥ìš©
        self.calibration_results = {
            'primary_camera_matrix': None,
            'primary_distortion': None,
            'secondary_camera_matrix': None,
            'secondary_distortion': None,
            'rotation_matrix': None,
            'translation_vector': None,
            'essential_matrix': None,
            'fundamental_matrix': None,
            'rectification_maps': None,
            'calibration_error': None,
            'calibration_date': None
        }
    
    def _create_camera_manager(self) -> CameraManager:
        """ì¹´ë©”ë¼ ê´€ë¦¬ì ìƒì„±"""
        try:
            if self.camera_type == "webcam":
                return create_webcam_manager(
                    device_id=self.camera_kwargs.get('device_id', 0)
                )
            elif self.camera_type == "zed":
                return create_zed_manager()
            elif self.camera_type == "azure_kinect":
                return create_azure_kinect_manager(
                    device_id=self.camera_kwargs.get('device_id', 0),
                    use_4k=self.camera_kwargs.get('use_4k', False)
                )
            elif self.camera_type == "dual_azure_kinect":
                return create_dual_azure_kinect_manager(
                    use_4k=self.camera_kwargs.get('use_4k', True)  # ë“€ì–¼ì€ ê¸°ë³¸ 4K
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´ë©”ë¼ íƒ€ì…: {self.camera_type}")
                
        except Exception as e:
            # ë¡œê±°ê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì¶œë ¥
            print(f"[ERROR] Camera Manager Creation Failed: {e}")
            raise
    
    def initialize_system(self) -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            print("[INFO] Initializing System...")
            
            # ì¹´ë©”ë¼ ì´ˆê¸°í™”
            if not self.camera_manager.initialize():
                print("[ERROR] Camera Initialization Failed")
                return False
            
            # ì¹´ë©”ë¼ ì •ë³´ ì¶œë ¥
            camera_info = self.camera_manager.get_camera_info()
            print(f"[INFO] Camera Initialized: {camera_info['camera_type']}")
            print(f"[INFO] Resolution: {camera_info['resolution']}")
            print(f"[INFO] FPS: {camera_info['fps']}")
            print(f"[INFO] Depth Sensor: {'Supported' if camera_info['use_depth'] else 'Not Supported'}")
            
            # Azure Kinect ì¶”ê°€ ì •ë³´
            if 'use_4k' in camera_info:
                print(f"[INFO] 4K Mode: {'Enabled' if camera_info['use_4k'] else 'Disabled'}")
            if 'secondary_available' in camera_info:
                print(f"[INFO] Dual Mode: {'2 Devices Connected' if camera_info['secondary_available'] else '1 Device Only'}")
            
            # ê¸°ì¡´ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” (Unity í†µì‹  í¬íŠ¸ í™•ì¸)
            unity_port = self._get_unity_port()
            print(f"[INFO] Unity Port: {unity_port}")
            
            # Body Tracking ì›Œì»¤ ì‹œì‘ (ë“€ì–¼ ëª¨ë“œì—ì„œ ë³´ì¡° ì¹´ë©”ë¼ ê¸°ì¤€)
            try:
                if self.camera_type == "dual_azure_kinect" and self.enable_bt:
                    self.bt_worker = BodyTrackingWorker(camera_id=1, target_fps=12)
                    # ê³µìœ  ìº¡ì²˜ ëª¨ë“œ í™œì„±í™”: CameraManagerì—ì„œ ì œê³µí•˜ëŠ” raw_capture ì‚¬ìš©
                    self.bt_worker.external_capture_mode = True
                    started = self.bt_worker.start()
                    print(f"[BT] Body Tracking: {'í™œì„±í™”' if started else 'ë¹„í™œì„±í™”'}")
                else:
                    print("[BT] Body Tracking: ë¹„í™œì„±í™” (í”Œë˜ê·¸ ë¯¸ì„¤ì •)")
                    self.bt_worker = None
            except Exception as e:
                print(f"[WARN] Body Tracking ì‹œì‘ ì‹¤íŒ¨: {e}")
            
            print("[INFO] System Initialization Complete!")
            return True
            
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _load_screen_plane_from_calibration(self) -> Optional[Dict]:
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì—ì„œ ìŠ¤í¬ë¦° í‰ë©´(ë²•ì„  n, í‰ë©´ì  P0)ì„ ë¡œë“œ"""
        try:
            with open('azure_kinect_3d_calibration.json', 'r') as f:
                calib = json.load(f)
            plane = calib.get('screen_plane', {})
            # ê¸°ëŒ€ êµ¬ì¡°: {'point': [x,y,z], 'normal': [nx,ny,nz]}
            if 'point' in plane and 'normal' in plane:
                return {
                    'point': tuple(map(float, plane['point'])),
                    'normal': tuple(map(float, plane['normal']))
                }
            # í´ë°±: corners_3dë¡œ í‰ë©´ ê³„ì‚°
            corners = plane.get('corners_3d') if isinstance(plane, dict) else None
            if corners and len(corners) >= 3:
                import numpy as np
                p0 = np.array(corners[0], dtype=np.float64)
                p1 = np.array(corners[1], dtype=np.float64)
                p2 = np.array(corners[2], dtype=np.float64)
                n = np.cross(p1 - p0, p2 - p0)
                norm = np.linalg.norm(n)
                if norm > 1e-9:
                    n = n / norm
                    return {'point': (float(p0[0]), float(p0[1]), float(p0[2])),
                            'normal': (float(n[0]), float(n[1]), float(n[2]))}
        except Exception:
            pass
        return None

    def _intersect_screen_plane_with_primary_ray(self, pixel_uv: Tuple[int,int]) -> Optional[Tuple[float,float,float]]:
        """1ë²ˆ ì¹´ë©”ë¼ í”½ì…€ë¡œë¶€í„° ìƒì„±í•œ ê´‘ì„ ê³¼ ìŠ¤í¬ë¦° í‰ë©´ì˜ êµì ì„ ê³„ì‚°"""
        try:
            if self.screen_plane is None:
                return None
            # ë‚´ë¶€ íŒŒë¼ë¯¸í„°(HD ì¶”ì •) â€“ ì‹¤ì œ ê°’ì´ ìˆë‹¤ë©´ êµì²´
            fx, fy = 1000.0, 1000.0
            cx, cy = 960.0, 540.0
            u, v = float(pixel_uv[0]), float(pixel_uv[1])
            # ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë°©í–¥ ë²¡í„°(d_cam) ê³„ì‚° í›„ ì •ê·œí™”
            d_cam = np.array([(u - cx)/fx, (v - cy)/fy, 1.0], dtype=np.float64)
            d_cam = d_cam / np.linalg.norm(d_cam)
            # 1ë²ˆ ì¹´ë©”ë¼ ì›ì ì„ ì›”ë“œ ì›ì ìœ¼ë¡œ ê°€ì •(ì™¸ë¶€íŒŒë¼ë¯¸í„° ë¯¸ì‚¬ìš© ì¼€ì´ìŠ¤)
            Cw = np.zeros(3, dtype=np.float64)
            Dw = d_cam
            P0 = np.array(self.screen_plane['point'], dtype=np.float64)
            n = np.array(self.screen_plane['normal'], dtype=np.float64)
            denom = np.dot(Dw, n)
            if abs(denom) < 1e-6:
                return None
            t = np.dot(P0 - Cw, n) / denom
            if t <= 0:
                return None
            Tw = Cw + t * Dw
            # mm ë‹¨ìœ„ ì‚¬ìš©(ê¸°ë³¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë‹¨ìœ„ì— ë§ì¶˜ë‹¤ê³  ê°€ì •)
            return (float(Tw[0]), float(Tw[1]), float(Tw[2]))
        except Exception:
            return None
    
    def _get_unity_port(self) -> int:
        """ì¹´ë©”ë¼ íƒ€ì…ì— ë”°ë¥¸ Unity í¬íŠ¸ ë°˜í™˜"""
        port_mapping = {
            'webcam': 12345,
            'zed': 9998,
            'azure_kinect': 9999,
            'dual_azure_kinect': 9997
        }
        return port_mapping.get(self.camera_type, 12345)
    
    def detect_laser_with_depth(self, color_frame: np.ndarray, depth_frame: Optional[np.ndarray]) -> LaserDetectionResult:
        """
        ê¹Šì´ ì •ë³´ë¥¼ í™œìš©í•œ ë ˆì´ì € ê²€ì¶œ (ê¸°ì¡´ laser_detector_core + ê¹Šì´ ì •ë³´)
        """
        start_time = time.time()
        
        # RGB í•˜ì´ë¸Œë¦¬ë“œ detector ì‚¬ìš© (Enhanced ì¼ì‹œ ë¹„í™œì„±í™”)
        # Enhanced DetectorëŠ” ì•„ì§ RGB ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŒ
        candidates = self.laser_core.detect_laser_candidates(color_frame, depth_frame)
        
        # ê°€ì¥ ì¢‹ì€ í›„ë³´ ì„ íƒ
        if not candidates:
            return LaserDetectionResult(
                detected=False,
                confidence=0.0,
                position=(0, 0),
                rgb_values=(0, 0, 0),
                brightness=0,
                detection_method="none",
                detection_time_ms=(time.time() - start_time) * 1000,
                screen_coordinate=(0.0, 0.0),
                unity_coordinate=(0.0, 0.0)
            )
        
        # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ í›„ë³´ ì„ íƒ
        best_candidate = max(candidates, key=lambda c: c.get('confidence', 0))
        
        # ê¹Šì´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì²˜ë¦¬
        if (depth_frame is not None and 
            self.azure_kinect_config['use_depth_filtering'] and 
            best_candidate.get('confidence', 0) > 0):
            
            x, y = best_candidate['position']
            
            try:
                # í•´ìƒë„ ë¶ˆì¼ì¹˜ í•´ê²°: 4K ì¢Œí‘œë¥¼ ê¹Šì´ ë§µ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
                depth_height, depth_width = depth_frame.shape[:2]
                
                # 4K í•´ìƒë„ì—ì„œ ê¹Šì´ ë§µ í•´ìƒë„ë¡œ ì¢Œí‘œ ë³€í™˜ (ì•ˆì „í•œ clamp ì ìš©)
                if self.camera_kwargs.get('use_4k', False):
                    color_height, color_width = 2160, 3840
                    scaled_x = max(0, min(int(x * depth_width / color_width), depth_width - 1))
                    scaled_y = max(0, min(int(y * depth_height / color_height), depth_height - 1))
                else:
                    scaled_x = max(0, min(x, depth_width - 1))
                    scaled_y = max(0, min(y, depth_height - 1))
                
                # ë””ë²„ê·¸ ë¡œê¹… (í•„ìš”ì‹œ)
                if x != scaled_x or y != scaled_y:
                    self.logger.debug(f"ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§: ({x}, {y}) â†’ ({scaled_x}, {scaled_y})")
                
                # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦ (ì´ì œ í•­ìƒ í†µê³¼í•´ì•¼ í•¨)
                if not (0 <= scaled_x < depth_width and 0 <= scaled_y < depth_height):
                    self.logger.warning(f"ê¹Šì´ ì¢Œí‘œ ë²”ìœ„ ì´ˆê³¼: ({scaled_x}, {scaled_y}) vs ({depth_width}, {depth_height})")
                    return LaserDetectionResult(
                        detected=best_candidate.get('confidence', 0) > 0,
                        confidence=best_candidate.get('confidence', 0),
                        position=best_candidate.get('position', (0, 0)),
                        rgb_values=best_candidate.get('rgb_values', (0, 0, 0)),
                        brightness=best_candidate.get('brightness', 0),
                        detection_method=f"{best_candidate.get('detection_method', 'unknown')}+depth_out_of_bounds",
                        detection_time_ms=(time.time() - start_time) * 1000,
                        screen_coordinate=best_candidate.get('screen_coordinate', (0.0, 0.0)),
                        unity_coordinate=best_candidate.get('unity_coordinate', (0.0, 0.0))
                    )
                
                # ê¹Šì´ ê°’ ì¶”ì¶œ (ìŠ¤ì¼€ì¼ë§ëœ ì¢Œí‘œ ì‚¬ìš©)
                depth_mm = float(depth_frame[scaled_y, scaled_x])
                
                # ìœ íš¨ ê¹Šì´ ë²”ìœ„ í™•ì¸
                min_depth, max_depth = self.azure_kinect_config['depth_range_mm']
                
                if min_depth <= depth_mm <= max_depth:
                    # ê¹Šì´ ê¸°ë°˜ ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                    depth_confidence_bonus = 0.2
                    enhanced_confidence = min(1.0, best_candidate.get('confidence', 0) + depth_confidence_bonus)
                    
                    # 3D ì›”ë“œ ì¢Œí‘œ ê³„ì‚° (ì›ë³¸ 4K ì¢Œí‘œ ì‚¬ìš©)
                    world_coords = self._pixel_to_world_coordinates(x, y, depth_mm)
                    
                    self.stats['depth_detections'] += 1
                    
                    return LaserDetectionResult(
                        detected=True,
                        confidence=enhanced_confidence,
                        position=(x, y),
                        rgb_values=best_candidate.get('rgb_values', (0, 0, 0)),
                        brightness=best_candidate.get('brightness', 0),
                        detection_method=f"{best_candidate.get('detection_method', 'unknown')}+depth",
                        detection_time_ms=(time.time() - start_time) * 1000,
                        screen_coordinate=best_candidate.get('screen_coordinate', (0.0, 0.0)),
                        unity_coordinate=best_candidate.get('unity_coordinate', (0.0, 0.0))
                    )
                else:
                    # ê¹Šì´ê°€ ìœ íš¨ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ê²€ì¶œ ë¬´íš¨í™”
                    return LaserDetectionResult(
                        detected=False,
                        confidence=0.0,
                        position=(0, 0),
                        rgb_values=(0, 0, 0),
                        brightness=0.0,
                        detection_method="depth_filtered_out",
                        detection_time_ms=(time.time() - start_time) * 1000
                    )
                    
            except (IndexError, ValueError) as e:
                self.logger.warning(f"ê¹Šì´ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ê¹Šì´ ì •ë³´ê°€ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
        return LaserDetectionResult(
            detected=best_candidate.get('confidence', 0) > 0,
            confidence=best_candidate.get('confidence', 0),
            position=best_candidate.get('position', (0, 0)),
            rgb_values=best_candidate.get('rgb_values', (0, 0, 0)),
            brightness=best_candidate.get('brightness', 0),
            detection_method=best_candidate.get('detection_method', 'unknown'),
            detection_time_ms=(time.time() - start_time) * 1000,
            screen_coordinate=best_candidate.get('screen_coordinate', (0.0, 0.0)),
            unity_coordinate=best_candidate.get('unity_coordinate', (0.0, 0.0))
        )
    
    def _pixel_to_world_coordinates(self, x: float, y: float, depth_mm: float) -> Tuple[float, float, float]:
        """í”½ì…€ ì¢Œí‘œë¥¼ 3D ì›”ë“œ ì¢Œí‘œë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•€í™€ ëª¨ë¸)"""
        # Azure Kinect ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì¶”í›„ ì‹¤ì œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ êµì²´)
        if self.camera_kwargs.get('use_4k', False):
            fx, fy = 2000.0, 2000.0  # 4Kìš© ì¶”ì •ê°’
            cx, cy = 1920.0, 1080.0
        else:
            fx, fy = 1000.0, 1000.0  # HDìš© ì¶”ì •ê°’
            cx, cy = 960.0, 540.0
        
        # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        world_x = (x - cx) * depth_mm / fx
        world_y = (y - cy) * depth_mm / fy
        world_z = depth_mm
        
        return (world_x, world_y, world_z)
    
    def detect_chessboard_corners(self, frame: np.ndarray, camera_name: str = "primary") -> Tuple[bool, np.ndarray]:
        """
        ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆ ê²€ì¶œ (ì—í”¼í´ë¼ì¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìš©)
        
        Args:
            frame: ì…ë ¥ í”„ë ˆì„
            camera_name: ì¹´ë©”ë¼ ì´ë¦„ ("primary" ë˜ëŠ” "secondary")
            
        Returns:
            (detected, corners): ê²€ì¶œ ì„±ê³µ ì—¬ë¶€ì™€ ì½”ë„ˆ ì¢Œí‘œë“¤
        """
        try:
            if frame is None:
                return False, np.array([])
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆ ê²€ì¶œ
            pattern_size = self.calibration_config['chessboard_size']
            found, corners = cv2.findChessboardCorners(
                gray, 
                pattern_size,
                cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK
            )
            
            if found and len(corners) >= self.calibration_config['min_corners_detected']:
                # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¡œ ì½”ë„ˆ ê°œì„ 
                criteria = self.calibration_config['subpix_criteria']
                corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
                
                # ì‹¤ì‹œê°„ ê²€ì¶œ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ëŠ” ë³„ë„ ìˆ˜ì§‘)
                if camera_name == "primary":
                    self.calibration_data['primary_detected'] = True
                else:
                    self.calibration_data['secondary_detected'] = True
                
                # ë¡œê·¸ ì¶œë ¥ ì™„ì „ ë¹„í™œì„±í™” (ë„ˆë¬´ ë§ì€ ë©”ì‹œì§€ ë°©ì§€)
                # self.logger.info(f"[SUCCESS] {camera_name} ì¹´ë©”ë¼ ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆ ê²€ì¶œ ì¤‘: {len(corners_refined)}ê°œ")
                return True, corners_refined
            
            else:
                # ê²€ì¶œ ì‹¤íŒ¨
                if camera_name == "primary":
                    self.calibration_data['primary_detected'] = False
                else:
                    self.calibration_data['secondary_detected'] = False
                
                return False, np.array([])
                
        except Exception as e:
            self.logger.error(f"{camera_name} ì¹´ë©”ë¼ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return False, np.array([])
    
    def draw_chessboard_corners(self, frame: np.ndarray, corners: np.ndarray, found: bool, 
                               scale_x: float = 1.0, scale_y: float = 1.0) -> np.ndarray:
        """
        ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
        
        Args:
            frame: ë””ìŠ¤í”Œë ˆì´ í”„ë ˆì„
            corners: ê²€ì¶œëœ ì½”ë„ˆë“¤
            found: ê²€ì¶œ ì„±ê³µ ì—¬ë¶€
            scale_x, scale_y: ë””ìŠ¤í”Œë ˆì´ ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨
            
        Returns:
            ì½”ë„ˆê°€ ê·¸ë ¤ì§„ í”„ë ˆì„
        """
        try:
            if found and len(corners) > 0:
                # ìŠ¤ì¼€ì¼ë§ ì ìš©ëœ ì½”ë„ˆë“¤
                scaled_corners = corners.copy()
                scaled_corners[:, 0, 0] *= scale_x
                scaled_corners[:, 0, 1] *= scale_y
                
                # ì²´ìŠ¤ë³´ë“œ íŒ¨í„´ ê·¸ë¦¬ê¸°
                pattern_size = self.calibration_config['chessboard_size']
                cv2.drawChessboardCorners(frame, pattern_size, scaled_corners, found)
                
                # ì½”ë„ˆ ê°œìˆ˜ í‘œì‹œ
                corner_count_text = f"Corners: {len(corners)}/{pattern_size[0] * pattern_size[1]}"
                cv2.putText(frame, corner_count_text, (10, 150), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # ì²« ë²ˆì§¸ ì½”ë„ˆì— ë²ˆí˜¸ í‘œì‹œ (ë°©í–¥ í™•ì¸ìš©)
                if len(scaled_corners) > 0:
                    first_corner = tuple(map(int, scaled_corners[0, 0]))
                    cv2.putText(frame, "1", first_corner, 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
            
            return frame
            
        except Exception as e:
            self.logger.error(f"ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return frame
    
    def process_dual_cameras(self) -> Optional[LaserDetectionResult]:
        """ë“€ì–¼ Azure Kinect ì¹´ë©”ë¼ ì²˜ë¦¬ + ì§€ëŠ¥í˜• ROI ì—…ë°ì´íŠ¸"""
        try:
            # ğŸ§  ì§€ëŠ¥í˜• ROI ì—…ë°ì´íŠ¸ (5í”„ë ˆì„ë§ˆë‹¤)
            self.roi_update_counter = getattr(self, 'roi_update_counter', 0) + 1
            if self.roi_update_counter >= self.roi_update_interval:
                self._update_intelligent_roi()
                self.roi_update_counter = 0
            
            # ğŸ“Œ í•µì‹¬: ë‹¨ì¼ ëª¨ë“œ ì„±ê³µ ë¡œì§ì„ ê·¸ëŒ€ë¡œ 2ë²ˆ ì‹¤í–‰
            
            # Camera #1: Primary (Screen) - ë‹¨ì¼ ëª¨ë“œì™€ ë™ì¼í•œ ë°©ì‹
            primary_frame_data = self.camera_manager.capture_frame()
            primary_result = None
            if primary_frame_data:
                # ì‹œê°í™”ìš© í”„ë ˆì„ ì„¤ì • (ì¤‘ìš”!)
                self.current_frame = primary_frame_data.color_frame
                self.current_depth = primary_frame_data.depth_frame
                self.current_image = primary_frame_data.color_frame.copy()
                
                # ë‹¨ì¼ ëª¨ë“œì™€ 100% ë™ì¼í•œ ê²€ì¶œ ë°©ì‹ (ì˜¬ë°”ë¥¸ ê¹Šì´ í”„ë ˆì„ ì‚¬ìš©)
                # ì¹´ë©”ë¼ë³„ í”„ë ˆì„ ì°¨ì´ ë²„í¼ ì‚¬ìš©: í˜¸ì¶œ ìˆœê°„ ì „ìš© í”„ë¡œì„¸ì„œë¡œ ìŠ¤ì™‘
                _prev_processor = self.frame_processor
                self.frame_processor = self.frame_processor_primary
                primary_result = self.detect_laser_with_motion_and_depth(
                    primary_frame_data.color_frame, 
                    primary_frame_data.depth_frame,
                    "screen",
                    core=self.laser_core_screen,
                    use_core_motion=True
                )
                self.frame_processor = _prev_processor
                primary_result.detection_method += "_camera0_primary"
            
            # Camera #2: Secondary (Gun + Screen) - ë‘ ê²½ë¡œ ë³‘í–‰  
            secondary_frame_data = self.camera_manager.capture_secondary_frame()
            secondary_result = None
            secondary_screen_result = None
            if secondary_frame_data:
                # ë³´ì¡° ì¹´ë©”ë¼ í”„ë ˆì„ ì„¤ì •
                self.secondary_frame = secondary_frame_data.color_frame
                self.secondary_depth = secondary_frame_data.depth_frame
                
                # ë‹¨ì¼ ëª¨ë“œì™€ 100% ë™ì¼í•œ ê²€ì¶œ ë°©ì‹ (ì¹´ë©”ë¼ë³„ í”„ë ˆì„ ì°¨ì´ ë²„í¼ ë¶„ë¦¬)
                _prev_processor = self.frame_processor
                self.frame_processor = self.frame_processor_secondary
                # ì´êµ¬ ì „ìš© ê°„ì ‘ê´‘/ì ê´‘ ê²€ì¶œ ê²½ë¡œ + BT ìœµí•©
                muzzle = self.muzzle_detector.detect(secondary_frame_data.color_frame,
                                                     secondary_frame_data.depth_frame)
                bt = None
                if self.enable_bt and self.bt_worker:
                    bt = self.bt_worker.get_latest_result()
                # ê°„ë‹¨ ìœµí•© ê·œì¹™: BTê°€ ìˆìœ¼ë©´ BT ì˜ˆì¸¡ ìœ„ì¹˜ ê·¼ì ‘ í›„ë³´ì— ë³´ë„ˆìŠ¤, ì—†ìœ¼ë©´ ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©
                if muzzle.detected:
                    pos = muzzle.position
                    conf = muzzle.confidence
                    if bt and bt.estimated_muzzle_3d is not None and bt.wrist_2d and bt.index_tip_2d:
                        # BT 2D ì˜ˆì¸¡ ê·¼ì‚¬: ì†ëª©-ê²€ì§€ ì¤‘ì  íˆ¬ì˜ê°’ì´ ìˆë‹¤ë©´ ê°€ì¤‘ì¹˜ ë³´ë„ˆìŠ¤
                        try:
                            px = (bt.wrist_2d[0] + bt.index_tip_2d[0]) / 2 if (bt.wrist_2d and bt.index_tip_2d) else None
                            py = (bt.wrist_2d[1] + bt.index_tip_2d[1]) / 2 if (bt.wrist_2d and bt.index_tip_2d) else None
                            if px is not None and py is not None:
                                dist = ((pos[0]-px)**2 + (pos[1]-py)**2) ** 0.5
                                if dist < 50:
                                    conf = min(1.0, conf + 0.1)
                        except Exception:
                            pass
                    secondary_result = LaserDetectionResult(
                        detected=True,
                        confidence=conf,
                        position=pos,
                        rgb_values=(0, 0, 0),
                        brightness=muzzle.brightness,
                        detection_method=("muzzle_detector_bt" if bt else "muzzle_detector"),
                        detection_time_ms=0.0,
                        screen_coordinate=(0, 0),
                        unity_coordinate=(0, 0, 0)
                    )
                else:
                    secondary_result = self.detect_laser_with_motion_and_depth(
                        secondary_frame_data.color_frame,
                        secondary_frame_data.depth_frame,
                        "gun",
                        core=self.laser_core_gun,
                        use_core_motion=True
                    )
                    # ê¹Šì´ ê·¼ì ‘ ê²Œì´íŠ¸: ì´êµ¬ëŠ” ê·¼ê±°ë¦¬(ì˜ˆ: 0.2~1.2m)ì— ì¡´ì¬í•´ì•¼ í•¨
                    try:
                        if secondary_result and secondary_result.detected and secondary_frame_data.depth_frame is not None:
                            cx, cy = map(int, secondary_result.position)
                            h, w = secondary_frame_data.depth_frame.shape[:2]
                            if 0 <= cx < w and 0 <= cy < h:
                                dx1, dy1 = max(0, cx - 2), max(0, cy - 2)
                                dx2, dy2 = min(w, cx + 3), min(h, cy + 3)
                                dpatch = secondary_frame_data.depth_frame[dy1:dy2, dx1:dx2].astype(np.float32)
                                valid = dpatch[(dpatch > 0) & (dpatch < 10000)]
                                depth_mm = float(np.median(valid)) if valid.size > 0 else -1.0
                                # MuzzleDetectorì™€ ë™ì¼í•œ ê·¼ì ‘ ë²”ìœ„ ì ìš©
                                near_mm, far_mm = 200.0, 1200.0
                                if depth_mm <= 0 or not (near_mm <= depth_mm <= far_mm):
                                    # ì›ê±°ë¦¬(ë²½/ìŠ¤í¬ë¦°) ë ˆì´ì € ìŠ¤íŒŸ ì˜¤ê²€ì¶œ ì°¨ë‹¨
                                    secondary_result.detected = False
                                    secondary_result.detection_method = "gun_rejected_by_depth"
                                else:
                                    secondary_result.depth_mm = depth_mm
                    except Exception:
                        pass

                # ë³´ì¡° ì¹´ë©”ë¼ì—ì„œë„ ìŠ¤í¬ë¦° íˆíŠ¸ì  ê²€ì¶œ ê²½ë¡œ ì¶”ê°€(ê¹Šì´ ë¯¸ì‚¬ìš©)
                try:
                    secondary_screen_result = self.detect_laser_with_motion_and_depth(
                        secondary_frame_data.color_frame,
                        None,
                        "screen",
                        core=self.laser_core_screen,
                        use_core_motion=True
                    )
                except Exception:
                    secondary_screen_result = None
                self.frame_processor = _prev_processor

                # BT ì™¸ë¶€ ìº¡ì²˜ íì— ê³µìœ  í”„ë ˆì„ enqueue (ê°€ëŠ¥í•  ë•Œë§Œ)
                try:
                    if self.enable_bt and self.bt_worker and getattr(self.bt_worker, 'external_capture_mode', False):
                        raw_cap = secondary_frame_data.camera_info and None
                        # CameraFrameì— raw_capture í•„ë“œê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©
                        raw_cap = getattr(secondary_frame_data, 'raw_capture', None)
                        if raw_cap is not None:
                            self.bt_worker.external_capture_queue.append(raw_cap)
                except Exception:
                    pass
                secondary_result.detection_method += "_camera1_secondary"
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['frames_processed'] += 1
            
            # ê²°ê³¼ í†µí•©(ë‹¨ìˆœ): ìŠ¤í¬ë¦°(primary) ìš°ì„ 
            final_result = primary_result if (primary_result and primary_result.detected) else secondary_result
            # ì „ì†¡: ì‚¼ê°ì¸¡ëŸ‰ ë¯¸ì‚¬ìš©. íƒ€ê²Ÿ 3D=ìŠ¤í¬ë¦° í‰ë©´ êµì°¨, ì´êµ¬ 3D=secondary ê¹Šì´+BT
            if (primary_result and primary_result.detected) or (secondary_result and secondary_result.detected):
                self.stats['detections'] += 1
                try:
                    target_world = None
                    if primary_result and primary_result.detected:
                        target_world = self._intersect_screen_plane_with_primary_ray(primary_result.position)

                    muzzle_world = None
                    if secondary_result and secondary_result.detected and hasattr(secondary_result, 'position'):
                        muzzle_world = self._compute_muzzle_world_from_secondary(
                            secondary_result.position, getattr(secondary_result, 'depth_mm', None)
                        )

                    if target_world is not None and muzzle_world is not None:
                        # 3D ë²¡í„° ê²€ì¦ ë¡œê·¸(ìš”ì•½): ê±°ë¦¬, ë°©í–¥ ìœ íš¨ì„±, ìŠ¤í¬ë¦° ë²•ì„ ê³¼ì˜ ê°ë„
                        try:
                            import numpy as _np
                            vec = _np.array(target_world) - _np.array(muzzle_world)
                            length = float(_np.linalg.norm(vec))
                            angle_deg = None
                            if self.screen_plane is not None:
                                n = _np.array(self.screen_plane['normal'], dtype=_np.float64)
                                vn = _np.dot(vec/ (length + 1e-6), n/ (_np.linalg.norm(n)+1e-6))
                                angle_deg = float(_np.degrees(_np.arccos(max(-1.0, min(1.0, vn)))))
                            self.logger.debug(f"[VECTOR] len={length:.1f}mm, angle_to_normal={angle_deg}")
                        except Exception:
                            pass
                        self._send_vector_from_points_millimeters(
                            muzzle_world, target_world,
                            confidence=(primary_result.confidence if primary_result else 0.5)
                        )
                except Exception as e:
                    self.logger.warning(f"ë“€ì–¼ ëª¨ë“œ Unity ì „ì†¡ ìŠ¤í‚µ: {e}")
            
            # ë³´ì¡° ì¹´ë©”ë¼ ê²°ê³¼ ì €ì¥ (ì‹œê°í™”ìš©)
            self._last_secondary_result = secondary_result
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"ë“€ì–¼ ì¹´ë©”ë¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _process_single_camera_pipeline(self, camera_id: int, camera_role: str) -> Optional[LaserDetectionResult]:
        """ë‹¨ì¼ ì¹´ë©”ë¼ íŒŒì´í”„ë¼ì¸ (ë“€ì–¼ ëª¨ë“œìš©)"""
        try:
            # ì¹´ë©”ë¼ë³„ í”„ë ˆì„ ìº¡ì²˜
            if camera_id == 0:
                # Primary ì¹´ë©”ë¼ (Screen ì „ìš©)
                frame_data = self.camera_manager.capture_frame()
                self._primary_frame = frame_data.color_frame if frame_data else None
            else:
                # Secondary ì¹´ë©”ë¼ (Gun ì „ìš©)  
                frame_data = self.camera_manager.capture_secondary_frame()
                self._secondary_frame = frame_data.color_frame if frame_data else None
            
            if not frame_data:
                return None
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ
            if self.calibration_only_mode or self.show_calibration:
                return LaserDetectionResult(
                    detected=False, confidence=0.0, position=(0, 0),
                    rgb_values=(0, 0, 0), brightness=0, detection_method="calibration_skip",
                    detection_time_ms=0.0, screen_coordinate=(0, 0), unity_coordinate=(0, 0, 0)
                )
            
            # ì¹´ë©”ë¼ë³„ ROI ì„¤ì • (ì—­í• ì— ë”°ë¼ ë¶„ë¦¬)
            if camera_role == "screen":
                # Screen ì¹´ë©”ë¼: ì¤‘ì•™ ì˜ì—­ ì§‘ì¤‘ (80% ì˜ì—­)
                height, width = frame_data.color_frame.shape[:2]
                screen_roi = (
                    int(width * 0.1),   # x1: 10% ë§ˆì§„
                    int(height * 0.1),  # y1: 10% ë§ˆì§„  
                    int(width * 0.9),   # x2: 90%ê¹Œì§€
                    int(height * 0.9)   # y2: 90%ê¹Œì§€
                )
                self.laser_core.set_roi("screen", screen_roi)
                
                # Screen: ê¹Šì´ í”„ë ˆì„ ë¬´ì‹œ (ìŠ¤í¬ë¦°ì€ ê¹Šì´ ì¸¡ì • ì–´ë ¤ì›€)
                result = self.detect_laser_with_motion_and_depth(
                    frame_data.color_frame, 
                    None,
                    "screen"
                )
                result.detection_method += f"_camera{camera_id}_screen"
                
            else:
                # Gun ì¹´ë©”ë¼: ì¢Œì¸¡ ìƒë‹¨ ì˜ì—­ ì§‘ì¤‘ (ì´êµ¬ ìœ„ì¹˜)
                height, width = frame_data.color_frame.shape[:2]
                gun_roi = (
                    0,                      # x1: ì¢Œì¸¡ë¶€í„°
                    0,                      # y1: ìƒë‹¨ë¶€í„°
                    int(width * 0.6),       # x2: 60%ê¹Œì§€
                    int(height * 0.6)       # y2: 60%ê¹Œì§€
                )
                self.laser_core.set_roi("gun", gun_roi)
                
                # Gun: ê¹Šì´ í”„ë ˆì„ í™œìš© (3D ì¢Œí‘œ í•„ìš”)
                result = self.detect_laser_with_motion_and_depth(
                    frame_data.color_frame,
                    frame_data.depth_frame,
                    "gun"
                )
                result.detection_method += f"_camera{camera_id}_gun"
            
            return result
            
        except Exception as e:
            self.logger.error(f"ë‹¨ì¼ ì¹´ë©”ë¼ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ (ID: {camera_id}): {e}")
            return None
    
    def _simple_dual_result_merge(self, primary_result: Optional[LaserDetectionResult], 
                                secondary_result: Optional[LaserDetectionResult]) -> Optional[LaserDetectionResult]:
        """ë‹¨ìˆœí•œ ë“€ì–¼ ê²°ê³¼ í†µí•© (ë‹¨ì¼ ëª¨ë“œ ìŠ¤íƒ€ì¼)"""
        try:
            # ë‘˜ ë‹¤ ê²€ì¶œëœ ê²½ìš°: ì‚¼ê°ì¸¡ëŸ‰ ë¹„ì‚¬ìš© â†’ ë” ë†’ì€ ì‹ ë¢°ë„ ë°˜í™˜
            if (primary_result and primary_result.detected and 
                secondary_result and secondary_result.detected):
                return primary_result if primary_result.confidence >= secondary_result.confidence else secondary_result
            
            # Screenë§Œ ê²€ì¶œëœ ê²½ìš° (ì¼ë°˜ì ì¸ ìƒí™©)
            elif primary_result and primary_result.detected:
                return primary_result
            
            # Gunë§Œ ê²€ì¶œëœ ê²½ìš°
            elif secondary_result and secondary_result.detected:
                return secondary_result
            
            # ë‘˜ ë‹¤ ê²€ì¶œ ì•ˆëœ ê²½ìš°
            else:
                return primary_result or LaserDetectionResult(
                    detected=False, confidence=0.0, position=(0, 0),
                    rgb_values=(0, 0, 0), brightness=0, detection_method="none",
                    detection_time_ms=0.0
                )
                
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return primary_result
    
    def _triangulate_3d_point(self, primary_result: LaserDetectionResult, 
                             secondary_result: LaserDetectionResult) -> Optional[LaserDetectionResult]:
        """ì‹¤ì œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ê¸°ë°˜ 3D ì‚¼ê°ì¸¡ëŸ‰"""
        try:
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° í™•ì¸
            if not hasattr(self, 'stereo_calibration') or not self.stereo_calibration:

                # ì„ì‹œ ì‚¼ê°ì¸¡ëŸ‰ (fallback)
                p1_x, p1_y = primary_result.position
                p2_x, p2_y = secondary_result.position
                triangulated_x = (p1_x + p2_x) / 2.0
                triangulated_y = (p1_y + p2_y) / 2.0
                triangulated_confidence = (primary_result.confidence + secondary_result.confidence) / 2.0
            else:
                # ì •í™•í•œ ì‚¼ê°ì¸¡ëŸ‰ ìˆ˜í–‰
                p1_x, p1_y = primary_result.position
                p2_x, p2_y = secondary_result.position
                
                # ìŠ¤í…Œë ˆì˜¤ ë¹„ì „ì„ í†µí•œ 3D ë³µì›
                world_3d = self._stereo_triangulate(
                    (p1_x, p1_y),  # Screen camera point
                    (p2_x, p2_y)   # Gun camera point
                )
                
                if world_3d:
                    # 3D ì¢Œí‘œê°€ ìˆìœ¼ë©´ ì´ë¥¼ í™œìš©
                    triangulated_x, triangulated_y = p1_x, p1_y  # Screen ê¸°ì¤€ ìœ ì§€
                    triangulated_confidence = (primary_result.confidence + secondary_result.confidence) / 2.0 + 0.1  # ë³´ë„ˆìŠ¤
                    print(f"[3D TRIANGULATION] World: {world_3d}")
                else:
                    # 3D ë³µì› ì‹¤íŒ¨ì‹œ fallback
                    triangulated_x = (p1_x + p2_x) / 2.0
                    triangulated_y = (p1_y + p2_y) / 2.0
                    triangulated_confidence = (primary_result.confidence + secondary_result.confidence) / 2.0
            
            final_result = LaserDetectionResult(
                detected=True,
                confidence=triangulated_confidence,
                position=(int(triangulated_x), int(triangulated_y)),
                rgb_values=primary_result.rgb_values,
                brightness=(primary_result.brightness + secondary_result.brightness) / 2.0,
                detection_method="dual_kinect_triangulation",
                detection_time_ms=max(primary_result.detection_time_ms, secondary_result.detection_time_ms),
                screen_coordinate=primary_result.screen_coordinate,
                unity_coordinate=primary_result.unity_coordinate
            )
            # 3D ì¢Œí‘œê°€ ê³„ì‚°ëœ ê²½ìš° ê²°ê³¼ ê°ì²´ì— ì €ì¥ (í›„ì† Unity ì „ì†¡ì„ ìœ„í•´ í•„ìš”)
            try:
                if 'world_3d' in locals() and world_3d is not None:
                    final_result.world_3d_point = tuple(np.asarray(world_3d).flatten().tolist())
            except Exception:
                pass
            
            # ê²°ê³¼ ìºì‹± (í”„ë ˆì„ ìŠ¤í‚µ ì‹œ ì¬ì‚¬ìš©)
            self._last_primary_result = final_result
            
            # FPS ì¸¡ì • ì™„ë£Œ
            frame_end_time = time.time()
            self.last_frame_time = frame_end_time
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"3D ì‚¼ê°ì¸¡ëŸ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _stereo_triangulate(self, point1: tuple, point2: tuple) -> Optional[tuple]:
        """ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¸°ë°˜ 3D ë³µì›"""
        try:
            import cv2
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ (ìµœì‹  íŒŒì¼)
            calib_file = "calibration_results/stereo_calibration_2025-08-06T20-40-25.json"
            
            try:
                with open(calib_file, 'r') as f:
                    import json
                    calib_data = json.load(f)
                    
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ (êµ¬ì¡° ë³€ê²½ ë°˜ì˜)
                camera_matrix_1 = np.array(calib_data['primary_camera_intrinsics']['camera_matrix'])
                camera_matrix_2 = np.array(calib_data['secondary_camera_intrinsics']['camera_matrix'])
                dist_coeffs_1 = np.array(calib_data['primary_camera_intrinsics']['distortion'][0])
                dist_coeffs_2 = np.array(calib_data['secondary_camera_intrinsics']['distortion'][0])
                R = np.array(calib_data['stereo_parameters']['rotation_matrix'])
                T = np.array(calib_data['stereo_parameters']['translation_vector']).reshape(3, 1)
                
                # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
                undistorted_1 = cv2.undistortPoints(
                    np.array([[point1]], dtype=np.float32), 
                    camera_matrix_1, 
                    dist_coeffs_1
                )[0][0]
                
                undistorted_2 = cv2.undistortPoints(
                    np.array([[point2]], dtype=np.float32), 
                    camera_matrix_2, 
                    dist_coeffs_2
                )[0][0]
                
                # 3D ì‚¼ê°ì¸¡ëŸ‰
                # P1 = [I|0], P2 = [R|T]
                P1 = np.hstack([np.eye(3), np.zeros((3, 1))])
                P2 = np.hstack([R, T])
                
                # ë™ì°¨ ì¢Œí‘œê³„ì—ì„œ 3D ì  ê³„ì‚°
                points_4d = cv2.triangulatePoints(
                    P1, P2, 
                    undistorted_1.reshape(2, 1), 
                    undistorted_2.reshape(2, 1)
                )
                
                # ì •ê·œí™”
                world_3d = points_4d[:3] / points_4d[3]
                
                print(f"[STEREO] 3D Point: {world_3d.flatten()}")
                return tuple(world_3d.flatten())
                
            except FileNotFoundError:
                print(f"[WARN] ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ ì—†ìŒ: {calib_file}")
                return None
            except Exception as e:
                print(f"[ERROR] ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
                
        except Exception as e:
            self.logger.error(f"ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰ ì‹¤íŒ¨: {e}")
            return None

    def _compute_muzzle_world_from_secondary(self, pixel_xy: tuple, depth_mm: Optional[float]) -> Optional[tuple]:
        """
        Secondary ì¹´ë©”ë¼ì˜ í”½ì…€ê³¼ ê¹Šì´(mm)ë¥¼ ì´ìš©í•´ Secondary ì¢Œí‘œê³„ì˜ 3D ì ì„ ë³µì›í•œ ë’¤
        Primary ì¢Œí‘œê³„ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•œë‹¤.
        """
        try:
            if depth_mm is None or depth_mm <= 0:
                return None
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° í•„ìš”
            if not hasattr(self, 'stereo_calibration') or not self.stereo_calibration:
                # ë¡œë“œ ì‹œë„
                _ = self._stereo_triangulate((0, 0), (0, 0))  # ë‚´ë¶€ ë¡œë”© ê²½ë¡œ ì¬ì‚¬ìš©
            import numpy as np
            import cv2
            calib_file = "calibration_results/stereo_calibration_2025-08-06T20-40-25.json"
            with open(calib_file, 'r') as f:
                import json
                calib_data = json.load(f)

            K2 = np.array(calib_data['secondary_camera_intrinsics']['camera_matrix'])
            dist2 = np.array(calib_data['secondary_camera_intrinsics']['distortion'][0])
            R = np.array(calib_data['stereo_parameters']['rotation_matrix'])
            T = np.array(calib_data['stereo_parameters']['translation_vector']).reshape(3, 1)

            # Secondary í”½ì…€ ì •ê·œí™”
            undistorted = cv2.undistortPoints(
                np.array([[pixel_xy]], dtype=np.float32), K2, dist2
            )[0][0]  # (x_n, y_n)

            z = depth_mm  # mm
            x = undistorted[0] * z
            y = undistorted[1] * z
            X2 = np.array([[x], [y], [z]])  # Secondary ì¢Œí‘œê³„ (mm)

            # Primary ì¢Œí‘œê³„ë¡œ ë³€í™˜: X1 = R^T * (X2 - T)
            X1 = R.T @ (X2 - T)
            return (float(X1[0, 0]), float(X1[1, 0]), float(X1[2, 0]))
        except Exception as e:
            self.logger.warning(f"ì´êµ¬ 3D ë³µì› ì‹¤íŒ¨: {e}")
            return None

    def _send_vector_from_points_millimeters(self, origin_mm: tuple, target_mm: tuple, confidence: float = 1.0) -> None:
        """ì›ì /íƒ€ê²Ÿ 3D(mm)ë¥¼ ë°›ì•„ Unity í˜¸í™˜ í¬ë§·(m)ìœ¼ë¡œ ì†¡ì‹ """
        try:
            vector_3d = {
                "message_type": "laser_3d_vector",
                "detected": True,
                "timestamp": time.time(),
                "origin": {"x": origin_mm[0] / 1000.0, "y": origin_mm[1] / 1000.0, "z": origin_mm[2] / 1000.0},
                "target": {"x": target_mm[0] / 1000.0, "y": target_mm[1] / 1000.0, "z": target_mm[2] / 1000.0},
                "direction": {
                    "x": float((target_mm[0] - origin_mm[0]) / max(1e-6, np.linalg.norm(np.array(target_mm) - np.array(origin_mm)))),
                    "y": float((target_mm[1] - origin_mm[1]) / max(1e-6, np.linalg.norm(np.array(target_mm) - np.array(origin_mm)))),
                    "z": float((target_mm[2] - origin_mm[2]) / max(1e-6, np.linalg.norm(np.array(target_mm) - np.array(origin_mm))))
                },
                "vector_length": float(np.linalg.norm(np.array(target_mm) - np.array(origin_mm))) / 1000.0,
                "confidence": float(confidence)
            }
            self._send_vector_to_unity(vector_3d)
        except Exception as e:
            self.logger.warning(f"ì›ì /íƒ€ê²Ÿ ì†¡ì‹  ì‹¤íŒ¨: {e}")
    
    def detect_laser_with_motion_and_depth(self, frame: np.ndarray, depth_frame: Optional[np.ndarray], roi_type: str = "screen", core: Optional[LaserDetectorCore] = None, use_core_motion: bool = False) -> LaserDetectionResult:
        """
        Azure Kinect RGB í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¶œ: ì›€ì§ì„ + ë°ê¸° + ê¹Šì´ ê²°í•© ê²€ì¶œ (HSV ì™„ì „ ì œê±°)
        
        í•µì‹¬ ê°œì„ :
        1. RGB í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¶œ â†’ ë°ê¸° + ì›€ì§ì„ + ê¹Šì´ ê²°í•© (ì¡°ëª… ë¬´ê´€)
        2. ì ì‘í˜• ì„ê³„ê°’ í•™ìŠµ â†’ í™˜ê²½ ë³€í™” ëŒ€ì‘
        3. í”„ë ˆì„ ì°¨ì´ ìµœìš°ì„  â†’ ë ˆì´ì € ON/OFF êµ¬ë¶„
        4. ë‹¨ì¼/ë“€ì–¼ ë™ì¼ ë¡œì§ â†’ ì¼ê´€ì„± ë³´ì¥
        """
        start_time = time.time()
        
        detection_result = LaserDetectionResult(
            detected=False, confidence=0.0, position=(0, 0),
            rgb_values=(0, 0, 0), brightness=0, detection_method="none",
            detection_time_ms=0.0, screen_coordinate=(0, 0), unity_coordinate=(0, 0, 0)
        )
        
        try:
            # ê¹Šì´ í”„ë ˆì„ ì„ íƒì  ì‚¬ìš© (ìŠ¤í¬ë¦° ëª¨ë“œ ëŒ€ì‘)
            if depth_frame is None:
                self.logger.debug("ê¹Šì´ í”„ë ˆì„ ì—†ìŒ - RGB ì „ìš© ëª¨ë“œë¡œ ê²€ì¶œ")
            
            # í”„ë ˆì„ ì°¨ì´ ê¸°ë°˜ ê²€ì¶œ (í•µì‹¬: ë‹¨ì¼ ì¹´ë©”ë¼ ì„±ê³µ ë°©ì‹)
            motion_mask = None
            if not use_core_motion and self.frame_processor.is_frame_diff_enabled():
                # 1ë‹¨ê³„: í”„ë ˆì„ ë²„í¼ ì—…ë°ì´íŠ¸
                self.frame_processor.update_frame_buffer(frame)
                
                # 2ë‹¨ê³„: ì›€ì§ì„ ì˜ì—­ ê²€ì¶œ (ë ˆì´ì € ON/OFF ìˆœê°„ ìºì¹˜)
                motion_result = self.frame_processor.detect_motion_regions(frame)
                motion_mask = motion_result.motion_mask
            
            # 3ë‹¨ê³„: ë ˆì´ì € ê²€ì¶œ (RGB ìš°ì„ , RGBD ë³´ì¡°)
            # ë‹¨ì¼ ëª¨ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì¶œ
            # ì¹´ë©”ë¼ë³„ ì½”ì–´ ì„ íƒ (ê¸°ë³¸ê°’: ë‹¨ì¼ ì½”ì–´)
            detect_core = core if core is not None else self.laser_core
            candidates = detect_core.detect_laser_candidates(
                frame, depth_frame, motion_mask, roi_type
            )
            
            if candidates:
                # ìµœê³  ì‹ ë¢°ë„ í›„ë³´ ì„ íƒ
                best_candidate = candidates[0]  # ì´ë¯¸ ì •ë ¬ë¨
                
                detection_result.detected = True
                # RGB í•˜ì´ë¸Œë¦¬ë“œì™€ í˜¸í™˜ì„± ì²˜ë¦¬
                if 'position' in best_candidate:
                    detection_result.position = best_candidate['position']
                elif 'center' in best_candidate:
                    detection_result.position = best_candidate['center']
                
                detection_result.confidence = best_candidate['confidence']
                detection_result.brightness = best_candidate['brightness']
                detection_result.detection_method = best_candidate.get('detection_method', 'rgbd')
                
                # 3D ì¢Œí‘œ í¬í•¨ (RGBDì˜ í•µì‹¬ ì¥ì )
                if 'world_3d' in best_candidate:
                    detection_result.world_3d_point = best_candidate['world_3d']
                if 'depth_mm' in best_candidate:
                    detection_result.depth_mm = best_candidate['depth_mm']
                
                # RGB ê°’ì„ ê²€ì¶œ ê²°ê³¼ì— í¬í•¨
                detection_result.rgb_values = best_candidate.get('rgb_values', (0, 0, 0))
                
                # ğŸ”¬ ê³¼í•™ì  ë ˆì´ì € ê²€ì¶œ ì„±ê³µ ë¡œê¹…
                position = best_candidate.get('position', best_candidate.get('center', (0, 0)))
                scientific_scores = best_candidate.get('scientific_scores', {})
                is_laser_candidate = best_candidate.get('is_green_laser', False)
                detection_method = best_candidate.get('detection_method', 'unknown')
                
                # ê³¼í•™ì  ê²€ì¶œ í”Œë˜ê·¸
                science_flag = "[scientific]" if detection_method == 'scientific_bayesian' else "[basic]"
                laser_flag = "[laser]" if is_laser_candidate else ""
                
                self.logger.debug(
                    f"{science_flag} ê²€ì¶œ ì„±ê³µ{laser_flag}: ìœ„ì¹˜={position}, "
                    f"ì‹ ë¢°ë„={best_candidate['confidence']:.2f}, "
                    f"ê¹Šì´={best_candidate.get('depth_mm', 0):.0f}mm, "
                    f"ë² ì´ì§€ì•ˆ={scientific_scores.get('bayesian_score', 0):.2f}, "
                    f"ë¬¼ë¦¬í•™ì ={scientific_scores.get('physics_score', 0):.2f}"
                )
            else:
                # RGBD ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ë¡œê¹…
                motion_status = "ëª¨ì…˜ ê²€ì¶œë¨" if motion_mask is not None else "ëª¨ì…˜ ì—†ìŒ"
                self.logger.debug(f"RGBD ê²€ì¶œ ì‹¤íŒ¨: {motion_status}")
            
        except Exception as e:
            self.logger.error(f"RGBD ê²€ì¶œ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œì—ë„ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (fallback ì œê±°)
        
        finally:
            processing_time = (time.time() - start_time) * 1000
            detection_result.detection_time_ms = processing_time
        
        return detection_result
    
    def process_frame(self) -> Optional[LaserDetectionResult]:
        """í”„ë ˆì„ ì²˜ë¦¬ ë©”ì¸ ë£¨í”„"""
        try:
            # ë“€ì–¼ ëª¨ë“œì¸ ê²½ìš° ë³„ë„ ì²˜ë¦¬
            if self.camera_type == "dual_azure_kinect":
                return self.process_dual_cameras()
            
            # ë‹¨ì¼ ì¹´ë©”ë¼ ì²˜ë¦¬
            frame_data = self.camera_manager.capture_frame()
            if not frame_data:
                return None
            
            self.current_frame = frame_data.color_frame
            self.current_depth = frame_data.depth_frame
            self.stats['frames_processed'] += 1
            
            # ë‹¨ì¼ ëª¨ë“œ: ìŠ¤í¬ë¦° í´ë¦¬ê³¤ì´ ì—†ìœ¼ë©´ 1íšŒ ìë™ ì ìš©(ë°”ë‹¥ ë°˜ì‚¬ ì°¨ë‹¨)
            try:
                if getattr(self.laser_core, 'screen_polygon', None) is None and not getattr(self, '_single_polygon_applied', False):
                    calib_base = (1920, 1080)
                    polygon = self._calculate_screen_polygon_from_calibration(calib_base[0], calib_base[1])
                    if polygon and len(polygon) >= 3:
                        cap_w, cap_h = frame_data.color_frame.shape[1], frame_data.color_frame.shape[0]
                        sx = cap_w / float(calib_base[0])
                        sy = cap_h / float(calib_base[1])
                        scaled_poly = [(int(x * sx), int(y * sy)) for (x, y) in polygon]
                        if hasattr(self.laser_core, 'set_screen_polygon'):
                            self.laser_core.set_screen_polygon(scaled_poly)
                        if hasattr(self.laser_core, 'set_roi_enable'):
                            # ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œê°€ ì—†ì„ ë•Œë§Œ í™œì„±
                            self.laser_core.set_roi_enable(True, user_override=False)
                        self._single_polygon_applied = True
                        print(f"[ROI] (ë‹¨ì¼) ìŠ¤í¬ë¦° í´ë¦¬ê³¤ ìë™ ì ìš©: {scaled_poly}")
            except Exception as e:
                print(f"[WARN] ë‹¨ì¼ ëª¨ë“œ ìŠ¤í¬ë¦° í´ë¦¬ê³¤ ì ìš© ì‹¤íŒ¨: {e}")

            # RGBD ë””ë²„ê¹…ì„ ìœ„í•œ í˜„ì¬ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
            self.current_image = frame_data.color_frame.copy()
            # RGB í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (HSV ì™„ì „ ì œê±°)
            
            # ë ˆì´ì € ê²€ì¶œ (ê¹Šì´ ì •ë³´ í™œìš©) - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ
            if self.calibration_only_mode or self.show_calibration:
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œì—ì„œëŠ” ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                detection_result = LaserDetectionResult(
                    detected=False, confidence=0.0, position=(0, 0),
                    rgb_values=(0, 0, 0), brightness=0, detection_method="calibration_skip",
                    detection_time_ms=0.0, screen_coordinate=(0, 0), unity_coordinate=(0, 0, 0)
                )
            else:
                # Azure Kinect ê°œì„ : ì›€ì§ì„ + RGB í•˜ì´ë¸Œë¦¬ë“œ ê²°í•© ê²€ì¶œ (HSV ì™„ì „ ì œê±°)
                detection_result = self.detect_laser_with_motion_and_depth(self.current_frame, self.current_depth, "screen")
            
            if detection_result.detected:
                self.stats['detections'] += 1
                
                # 3D ë²¡í„° ìƒì„± ë° Unity ì „ì†¡ (ê°œì„ ëœ ë²„ì „)
                try:
                    # 3D ì¢Œí‘œê°€ ìˆëŠ” ê²½ìš° ë²¡í„° ìƒì„±
                    if detection_result.world_3d_point and detection_result.depth_mm:
                        vector_3d = self._create_3d_vector(detection_result)
                        if vector_3d:
                            self._send_vector_to_unity(vector_3d)
                            self.stats['triangulated_points'] += 1
                    
                    # 2D ë§¤í•‘ë„ ìœ ì§€ (í˜¸í™˜ì„±)
                    screen_coord = self.screen_mapper.map_to_screen(detection_result.position)
                    detection_result.screen_coordinate = screen_coord
                    
                    try:
                        unity_coord = self.screen_mapper.convert_to_unity_coordinates(detection_result.position)
                        detection_result.unity_coordinate = unity_coord
                    except AttributeError:
                        # ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ í”½ì…€ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        detection_result.unity_coordinate = detection_result.position
                    
                except Exception as e:
                    self.logger.error(f"3D ë²¡í„° ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ìºì‹± (ë‹¨ì¼ ì¹´ë©”ë¼ìš©)
            self._last_primary_result = detection_result
            return detection_result
            
        except Exception as e:
            self.logger.error(f"í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_3d_vector(self, detection_result) -> Optional[Dict]:
        """3D ë²¡í„° ìƒì„± (ë‹¨ì¼ ì¹´ë©”ë¼ìš©)"""
        try:
            if not detection_result.world_3d_point:
                return None
            
            # ì¹´ë©”ë¼ ì›ì  (Azure Kinect ê¸°ì¤€)
            camera_origin = np.array([0.0, 0.0, 0.0])  # ì¹´ë©”ë¼ ìœ„ì¹˜
            
            # ë ˆì´ì € íƒ€ê²Ÿ 3D ì¢Œí‘œ
            target_3d = np.array(detection_result.world_3d_point)
            
            # ë²¡í„° ê³„ì‚°
            direction_vector = target_3d - camera_origin
            vector_magnitude = np.linalg.norm(direction_vector)
            
            if vector_magnitude > 0:
                direction_normalized = direction_vector / vector_magnitude
            else:
                return None
            
            # Unity ì „ì†¡ìš© ë²¡í„° ë°ì´í„° (ê¸°ì¡´ Unity ìŠ¤í¬ë¦½íŠ¸ì™€ í˜¸í™˜)
            vector_3d = {
                "message_type": "laser_3d_vector",  # Unity_Simple_Target_Hit_Systemê³¼ í˜¸í™˜
                "detected": True,
                "timestamp": time.time(),
                "origin": {
                    "x": camera_origin[0] / 1000.0,  # mm -> m
                    "y": camera_origin[1] / 1000.0,
                    "z": camera_origin[2] / 1000.0
                },
                "target": {
                    "x": target_3d[0] / 1000.0,  # mm -> m  
                    "y": target_3d[1] / 1000.0,
                    "z": target_3d[2] / 1000.0
                },
                "direction": {
                    "x": float(direction_normalized[0]),
                    "y": float(direction_normalized[1]),
                    "z": float(direction_normalized[2])
                },
                "vector_length": vector_magnitude / 1000.0,  # mm -> m
                "confidence": detection_result.confidence,
                # ì¶”ê°€ ë””ë²„ê·¸ ì •ë³´
                "depth_mm": detection_result.depth_mm,
                "detection_method": detection_result.detection_method,
                "pixel_position": detection_result.position
            }
            
            return vector_3d
            
        except Exception as e:
            self.logger.error(f"3D ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def _send_vector_to_unity(self, vector_3d: Dict):
        """Unityë¡œ 3D ë²¡í„° ì „ì†¡"""
        try:
            # JSON ì§ë ¬í™” ë° UDP ì „ì†¡
            json_data = json.dumps(vector_3d, separators=(',', ':'))
            message_bytes = json_data.encode('utf-8')
            
            # ì „ì†¡ í¬íŠ¸: ScreenMapper ì„¤ì •ì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ 9997 ê¸°ë³¸ê°’
            target_port = 9997
            try:
                if hasattr(self, 'screen_mapper') and hasattr(self.screen_mapper, 'unity_port'):
                    target_port = int(self.screen_mapper.unity_port)
            except Exception:
                target_port = 9997

            # UDP ì†Œì¼“ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ì„ì‹œ ì†Œì¼“ ì‚¬ìš©
            if hasattr(self.screen_mapper, 'udp_socket') and self.screen_mapper.udp_socket:
                self.screen_mapper.udp_socket.sendto(message_bytes, ("127.0.0.1", target_port))
                self.logger.debug(f"3D ë²¡í„° Unity ì „ì†¡ ì„±ê³µ: {len(message_bytes)} bytes")
            else:
                # UDP ì†Œì¼“ì´ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
                import socket
                udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                udp_socket.sendto(message_bytes, ("127.0.0.1", target_port))
                udp_socket.close()
                self.logger.debug(f"ì„ì‹œ ì†Œì¼“ìœ¼ë¡œ 3D ë²¡í„° ì „ì†¡ ì™„ë£Œ (port={target_port})")
                
        except Exception as e:
            self.logger.error(f"Unity 3D ë²¡í„° ì „ì†¡ ì‹¤íŒ¨: {e}")

    def _send_primary_pixel_ray(self, primary_result: 'LaserDetectionResult') -> None:
        """
        ì‚¼ê°ì¸¡ëŸ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ Unityì—ì„œ ì •í•©ì„ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•œ ë””ë²„ê·¸ìš© ë ˆì´ ì†¡ì‹ .
        - ì¹´ë©”ë¼ ì›ì (0,0,0)ì—ì„œ Primary í”½ì…€ ë°©í–¥ìœ¼ë¡œ ë‹¨ìœ„ë²¡í„°ë¥¼ ë§Œë“¤ì–´ ë³´ëƒ…ë‹ˆë‹¤.
        - ë‹¨ìœ„: m, ë©”ì‹œì§€ íƒ€ì…ì€ laser_3d_vector ê·¸ëŒ€ë¡œ.
        """
        try:
            import numpy as np
            # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ ì„ì˜ì˜ ì „ì§„ ë°©í–¥ ì‚¬ìš© (Z+), ê¸¸ì´ëŠ” fallback
            camera_origin = np.array([0.0, 0.0, 0.0])
            # í™”ë©´ ì •í•© í™•ì¸ ëª©ì ì´ë¯€ë¡œ ì „ë°© ë‹¨ìœ„ë²¡í„° ì‚¬ìš©
            direction_normalized = np.array([0.0, 0.0, 1.0])
            vector_3d = {
                "message_type": "laser_3d_vector",
                "detected": True,
                "timestamp": time.time(),
                "origin": {"x": 0.0, "y": 0.0, "z": 0.0},
                "target": {"x": 0.0, "y": 0.0, "z": 5.0},  # ì„ì‹œ 5m
                "direction": {
                    "x": float(direction_normalized[0]),
                    "y": float(direction_normalized[1]),
                    "z": float(direction_normalized[2])
                },
                "vector_length": 5.0,
                "confidence": max(0.0, float(primary_result.confidence)),
                "pixel_position": primary_result.position,
                "detection_method": primary_result.detection_method
            }
            self._send_vector_to_unity(vector_3d)
            # ë””ë²„ê·¸ ë ˆì´ ë¡œê·¸ëŠ” ì†ŒìŒì´ ë§ì•„ INFOì—ì„œ DEBUGë¡œ ë‚´ë¦¼
            self.logger.debug("ì‚¼ê°ì¸¡ëŸ‰ ì‹¤íŒ¨ - Primary í”½ì…€ ê¸°ë°˜ ë””ë²„ê·¸ ë ˆì´ ì†¡ì‹ ")
        except Exception as e:
            self.logger.warning(f"ë””ë²„ê·¸ ë ˆì´ ì†¡ì‹  ì‹¤íŒ¨: {e}")
    
    def visualize_frame(self, detection_result: Optional[LaserDetectionResult]) -> np.ndarray:
        """í”„ë ˆì„ ì‹œê°í™” (í™”ë©´ í¬ê¸° ì¡°ì •)"""
        if self.current_frame is None:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        # ì›ë³¸ í”„ë ˆì„ì„ ë””ìŠ¤í”Œë ˆì´ìš© í¬ê¸°ë¡œ ì¡°ì • (4K â†’ HD)
        original_frame = self.current_frame.copy()
        display_height, display_width = 720, 1280  # HD í¬ê¸°ë¡œ ì¡°ì •
        
        # 4Kì—ì„œ HDë¡œ ìŠ¤ì¼€ì¼ë§
        if original_frame.shape[1] > display_width:
            display_frame = cv2.resize(original_frame, (display_width, display_height))
            # ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨ ê³„ì‚° (ê²€ì¶œ ê²°ê³¼ ì¢Œí‘œ ì¡°ì •ìš©)
            self.display_scale_x = display_width / original_frame.shape[1]
            self.display_scale_y = display_height / original_frame.shape[0]
        else:
            display_frame = original_frame
            self.display_scale_x = 1.0
            self.display_scale_y = 1.0
        
        # ì¹´ë©”ë¼ ì •ë³´ í‘œì‹œ
        camera_info = self.camera_manager.get_camera_info()
        if self.camera_type == "dual_azure_kinect":
            cv2.putText(display_frame, "Camera #1 (Screen Detection)", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            cv2.putText(display_frame, f"Camera: {camera_info['camera_type']}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Azure Kinect ì¶”ê°€ ì •ë³´
        if 'use_4k' in camera_info:
            mode_text = "4K" if camera_info['use_4k'] else "HD"
            cv2.putText(display_frame, f"Mode: {mode_text}", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
        
        if 'secondary_available' in camera_info:
            dual_text = "Dual" if camera_info['secondary_available'] else "Single"
            cv2.putText(display_frame, f"Config: {dual_text}", 
                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 1)
        
        # ê²€ì¶œ ê²°ê³¼ ì‹œê°í™” (ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ ì ìš©)
        if detection_result and detection_result.detected:
            x, y = detection_result.position
            confidence = detection_result.confidence
            
            # ë””ìŠ¤í”Œë ˆì´ìš© ì¢Œí‘œë¡œ ë³€í™˜
            display_x = int(x * self.display_scale_x)
            display_y = int(y * self.display_scale_y)
            
            # ë ˆì´ì € í¬ì¸íŠ¸ í‘œì‹œ
            color = (0, 255, 0) if confidence > 0.7 else (0, 255, 255)
            cv2.circle(display_frame, (display_x, display_y), 10, color, 2)
            
            # ì •ë³´ í‘œì‹œ
            info_text = f"Conf: {confidence:.2f}"
            cv2.putText(display_frame, info_text, (display_x + 15, display_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
            
            # ì›ë³¸ ì¢Œí‘œ í‘œì‹œ (ë””ë²„ê·¸ìš©)
            coord_text = f"({x}, {y})"
            cv2.putText(display_frame, coord_text, (display_x + 15, display_y + 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
            
            # ê²€ì¶œ ë°©ë²• í‘œì‹œ
            method_text = f"Method: {detection_result.detection_method}"
            cv2.putText(display_frame, method_text, (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ)
        if self.show_calibration and self.current_frame is not None:
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œì—ì„œëŠ” ë§¤ í”„ë ˆì„ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ (ì‹¤ì‹œê°„ ì‹œê°í™”)
            found, corners = self.detect_chessboard_corners(self.current_frame, "primary")
            if found:
                display_frame = self.draw_chessboard_corners(
                    display_frame, corners, found, 
                    self.display_scale_x, self.display_scale_y
                )
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒíƒœ í‘œì‹œ (ê°œì„ ëœ ë²„ì „)
            primary_status = "OK" if self.calibration_data['primary_detected'] else "X"
            secondary_status = "OK" if self.calibration_data['secondary_detected'] else "X"
            collected = self.calibration_data['image_count']
            required = self.calibration_data['min_images_required']
            
            # ê°„ë‹¨í•˜ê³  ê¹”ë”í•œ ìƒíƒœ í‘œì‹œ
            status_text = f"Calib: P:{primary_status} S:{secondary_status} | Images: {collected}/{required}"
            cv2.putText(display_frame, status_text, (10, 180), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
            
            # ìˆ˜ì§‘ ì§„í–‰ë¥  ë°”
            if required > 0:
                bar_width = 200
                bar_height = 10
                progress = min(collected / required, 1.0)
                cv2.rectangle(display_frame, (10, 200), (10 + bar_width, 200 + bar_height), (100, 100, 100), -1)
                cv2.rectangle(display_frame, (10, 200), (10 + int(bar_width * progress), 200 + bar_height), (0, 255, 0), -1)
        
        # í†µê³„ í‘œì‹œ
        elapsed = time.time() - self.stats['start_time']
        if elapsed > 0:
            self.stats['fps'] = self.stats['frames_processed'] / elapsed
        
        cv2.putText(display_frame, f"FPS: {self.stats['fps']:.1f}", 
                   (10, display_frame.shape[0] - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Detections: {self.stats['detections']}", 
                   (10, display_frame.shape[0] - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        cv2.putText(display_frame, f"Depth Det: {self.stats['depth_detections']}", 
                   (10, display_frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # ë“€ì–¼ ëª¨ë“œ í†µê³„
        if self.camera_type == "dual_azure_kinect":
            cv2.putText(display_frame, f"3D Points: {self.stats['triangulated_points']}", 
                       (200, display_frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
        
        # RGB ëª¨ë“œì—ì„œëŠ” ê¹Šì´ ë§µ ì°½ ì œê±° (ë¶ˆí•„ìš”)
        
        # ë””ë²„ê·¸ ëª¨ë“œ: ëª¨ì…˜ ê²€ì¶œ ì‹œê°í™”
        if self.show_debug and hasattr(self.frame_processor, 'get_latest_motion_mask'):
            motion_mask = self.frame_processor.get_latest_motion_mask()
            if motion_mask is not None:
                # ëª¨ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì»¬ëŸ¬ë¡œ ë³€í™˜
                motion_display = cv2.applyColorMap(motion_mask, cv2.COLORMAP_HOT)
                
                # ëª¨ì…˜ í†µê³„ ì •ë³´ í‘œì‹œ
                motion_pixels = cv2.countNonZero(motion_mask)
                total_pixels = motion_mask.shape[0] * motion_mask.shape[1]
                motion_ratio = motion_pixels / total_pixels * 100
                
                cv2.putText(motion_display, f"Motion Pixels: {motion_pixels} ({motion_ratio:.1f}%)", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(motion_display, "Hot areas = Motion detected", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ëª¨ì…˜ ë§ˆìŠ¤í¬ ì°½ í‘œì‹œ (ì´ˆê¸° ìœ„ì¹˜ë§Œ ì„¤ì •)
                cv2.namedWindow("Motion Detection Analysis", cv2.WINDOW_NORMAL)
                if not hasattr(self, '_motion_window_positioned'):
                    cv2.moveWindow("Motion Detection Analysis", 50, 600)
                    cv2.resizeWindow("Motion Detection Analysis", 480, 270)
                    self._motion_window_positioned = True
                cv2.imshow("Motion Detection Analysis", motion_display)
        
        return display_frame
    
    def visualize_secondary_frame(self, secondary_result: Optional[LaserDetectionResult]) -> np.ndarray:
        """ë³´ì¡° ì¹´ë©”ë¼ í”„ë ˆì„ ì‹œê°í™” (ë“€ì–¼ ëª¨ë“œ ì „ìš©, í™”ë©´ í¬ê¸° ì¡°ì •)"""
        if self.secondary_frame is None:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        # ì›ë³¸ í”„ë ˆì„ì„ ë””ìŠ¤í”Œë ˆì´ìš© í¬ê¸°ë¡œ ì¡°ì • (4K â†’ HD)
        original_frame = self.secondary_frame.copy()
        display_height, display_width = 720, 1280  # HD í¬ê¸°ë¡œ ì¡°ì •
        
        # 4Kì—ì„œ HDë¡œ ìŠ¤ì¼€ì¼ë§
        if original_frame.shape[1] > display_width:
            display_frame = cv2.resize(original_frame, (display_width, display_height))
            # ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨ ê³„ì‚°
            secondary_scale_x = display_width / original_frame.shape[1]
            secondary_scale_y = display_height / original_frame.shape[0]
        else:
            display_frame = original_frame
            secondary_scale_x = 1.0
            secondary_scale_y = 1.0
        
        # ë³´ì¡° ì¹´ë©”ë¼ ì •ë³´ í‘œì‹œ
        cv2.putText(display_frame, "Camera #2 (Gun Detection)", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
        
        # Azure Kinect ì¶”ê°€ ì •ë³´
        camera_info = self.camera_manager.get_camera_info()
        if 'use_4k' in camera_info:
            mode_text = "4K" if camera_info['use_4k'] else "HD"
            cv2.putText(display_frame, f"Mode: {mode_text}", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
        
        cv2.putText(display_frame, "Secondary Camera", 
                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 1)
        
        # ê²€ì¶œ ê²°ê³¼ ì‹œê°í™” (ë³´ì¡° ì¹´ë©”ë¼ìš©, ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ ì ìš©)
        if secondary_result and secondary_result.detected:
            x, y = secondary_result.position
            confidence = secondary_result.confidence
            
            # ë””ìŠ¤í”Œë ˆì´ìš© ì¢Œí‘œë¡œ ë³€í™˜
            display_x = int(x * secondary_scale_x)
            display_y = int(y * secondary_scale_y)
            
            # ë ˆì´ì € í¬ì¸íŠ¸ í‘œì‹œ (ë³´ì¡° ì¹´ë©”ë¼ëŠ” ë‹¤ë¥¸ ìƒ‰ìƒ)
            color = (255, 0, 255) if confidence > 0.7 else (255, 255, 0)
            cv2.circle(display_frame, (display_x, display_y), 10, color, 2)
            
            # ì •ë³´ í‘œì‹œ
            info_text = f"Gun Conf: {confidence:.2f}"
            cv2.putText(display_frame, info_text, (display_x + 15, display_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
            
            # ì›ë³¸ ì¢Œí‘œ í‘œì‹œ (ë””ë²„ê·¸ìš©)
            coord_text = f"({x}, {y})"
            cv2.putText(display_frame, coord_text, (display_x + 15, display_y + 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ ì²˜ë¦¬ (ë³´ì¡° ì¹´ë©”ë¼) - ì‹¤ì‹œê°„ ê²€ì¶œ
        if self.show_calibration and self.secondary_frame is not None:
            # ë³´ì¡° ì¹´ë©”ë¼ë„ ë§¤ í”„ë ˆì„ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ (ì‹¤ì‹œê°„ ì‹œê°í™”)
            found, corners = self.detect_chessboard_corners(self.secondary_frame, "secondary")
            if found:
                display_frame = self.draw_chessboard_corners(
                    display_frame, corners, found, 
                    secondary_scale_x, secondary_scale_y
                )
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒíƒœ í‘œì‹œ (ê°œì„ ëœ ë²„ì „ - Secondary)
            primary_status = "OK" if self.calibration_data['primary_detected'] else "X"
            secondary_status = "OK" if self.calibration_data['secondary_detected'] else "X"
            collected = self.calibration_data['image_count']
            required = self.calibration_data['min_images_required']
            
            # ê°„ë‹¨í•˜ê³  ê¹”ë”í•œ ìƒíƒœ í‘œì‹œ
            status_text = f"Calib: P:{primary_status} S:{secondary_status} | Images: {collected}/{required}"
            cv2.putText(display_frame, status_text, (10, 180), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)
            
            # ìˆ˜ì§‘ ì§„í–‰ë¥  ë°”
            if required > 0:
                bar_width = 200
                bar_height = 10
                progress = min(collected / required, 1.0)
                cv2.rectangle(display_frame, (10, 200), (10 + bar_width, 200 + bar_height), (100, 100, 100), -1)
                cv2.rectangle(display_frame, (10, 200), (10 + int(bar_width * progress), 200 + bar_height), (255, 0, 255), -1)
        
        # ë³´ì¡° ì¹´ë©”ë¼ ì „ìš© í†µê³„ í‘œì‹œ
        cv2.putText(display_frame, "Gun Detection Mode", 
                   (10, display_frame.shape[0] - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
        
        # RGB ëª¨ë“œì—ì„œëŠ” ê¹Šì´ ë§µ ì°½ ì œê±° (ë¶ˆí•„ìš”)
        
        return display_frame
    
    def handle_keyboard_input(self, key: int):
        """í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬"""
        if key == ord('d') or key == ord('D'):
            self.show_debug = not self.show_debug
            print(f"ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if self.show_debug else 'ë¹„í™œì„±í™”'}")
            if self.show_debug:
                print("  - ëª¨ì…˜ ë§ˆìŠ¤í¬ ì‹œê°í™” í™œì„±í™”")
                print("  - ê²€ì¶œ ì˜ì—­ ë¶„ì„ í™œì„±í™”")
                print("  - í”„ë ˆì„ ì°¨ì´ ì„¸ë¶€ ì •ë³´ í‘œì‹œ")
        
        elif key == ord('z') or key == ord('Z'):
            self.show_depth = not self.show_depth
            print(f"ğŸ“ ê¹Šì´ ë§µ í‘œì‹œ: {'í™œì„±í™”' if self.show_depth else 'ë¹„í™œì„±í™”'}")
        
        elif key == ord('t') or key == ord('T'):
            if self.camera_type == "dual_azure_kinect":
                self.azure_kinect_config['triangulation_enabled'] = not self.azure_kinect_config['triangulation_enabled']
                print(f"ğŸ¯ 3D ì‚¼ê°ì¸¡ëŸ‰: {'í™œì„±í™”' if self.azure_kinect_config['triangulation_enabled'] else 'ë¹„í™œì„±í™”'}")
        
        elif key == ord('f') or key == ord('F'):
            self.azure_kinect_config['use_depth_filtering'] = not self.azure_kinect_config['use_depth_filtering']
            print(f"ğŸ” ê¹Šì´ í•„í„°ë§: {'í™œì„±í™”' if self.azure_kinect_config['use_depth_filtering'] else 'ë¹„í™œì„±í™”'}")
        
        elif key == ord('h') or key == ord('H'):
            # Enhanced CHT í† ê¸€ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
            if self.enhanced_laser_core is not None:
                current_cht = self.enhanced_laser_core.enable_cht
                self.enhanced_laser_core.toggle_cht(not current_cht)
                print(f"ğŸ” Modified CHT: {'í™œì„±í™”' if not current_cht else 'ë¹„í™œì„±í™”'}")
            else:
                print("â„¹ï¸ Enhanced Detectorê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        elif key == ord('e') or key == ord('E'):
            # Enhanced Detector í†µê³„ ì¶œë ¥ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
            if self.enhanced_laser_core is not None:
                stats = self.enhanced_laser_core.get_enhanced_stats()
                print("ğŸ“Š Enhanced Detector í†µê³„:")
                print(f"   CHT í™œì„±í™”: {stats['cht_enabled']}")
                print(f"   CHT ê²€ì¦ íšŸìˆ˜: {stats['cht_verifications']}")
                print(f"   CHT ê°œì„  íšŸìˆ˜: {stats['cht_improvements']}")
                print(f"   CHT ì„±ê³µë¥ : {stats['cht_success_rate']:.1f}%")
                print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_processing_time_ms']:.2f}ms")
                if 'cht_avg_time_ms' in stats:
                    print(f"   CHT í‰ê·  ì‹œê°„: {stats['cht_avg_time_ms']:.2f}ms")
                    print(f"   CHT ìµœëŒ€ ì‹œê°„: {stats['cht_max_time_ms']:.2f}ms")
            else:
                print("â„¹ï¸ Enhanced Detectorê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        elif key == ord('c') or key == ord('C'):
            self.show_calibration = not self.show_calibration
            print(f"ğŸ¯ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ: {'í™œì„±í™”' if self.show_calibration else 'ë¹„í™œì„±í™”'}")
            if self.show_calibration:
                print("ğŸ“‹ ì²´ìŠ¤ë³´ë“œ íŒ¨í„´ì„ ë‘ ì¹´ë©”ë¼ì— ëª¨ë‘ ë³´ì—¬ì£¼ì„¸ìš”")
                print(f"   íŒ¨í„´ í¬ê¸°: {self.calibration_config['chessboard_size'][0]}x{self.calibration_config['chessboard_size'][1]} ë‚´ë¶€ ì½”ë„ˆ")
                print("ğŸ“¸ 'S' í‚¤: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ (20ì¥ í•„ìš”)")
                print("ğŸ”¬ 'P' í‚¤: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ìˆ˜í–‰")
                print("ğŸš€ ì„±ëŠ¥ ìµœì í™”: ë ˆì´ì € ê²€ì¶œ ë¹„í™œì„±í™”ë¨ (FPS í–¥ìƒ)")
        
        elif key == ord('s') or key == ord('S'):
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘
            if self.show_calibration and self.camera_type == "dual_azure_kinect":
                self.capture_calibration_image()
        
        elif key == ord('p') or key == ord('P'):
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ìˆ˜í–‰
            if self.show_calibration and self.camera_type == "dual_azure_kinect":
                self.perform_stereo_calibration()
        
        elif key == ord('l') or key == ord('L'):
            # ë¡œê·¸ ë ˆë²¨ ì¡°ì •
            current_level = self.logger.level
            if current_level == 20:  # INFO
                self.logger.setLevel(30)  # WARNING
                print("ğŸ“ ë¡œê·¸ ë ˆë²¨: WARNING (INFO ë©”ì‹œì§€ ìˆ¨ê¹€)")
            elif current_level == 30:  # WARNING
                self.logger.setLevel(40)  # ERROR
                print("ğŸ“ ë¡œê·¸ ë ˆë²¨: ERROR (ê²½ê³  ë©”ì‹œì§€ë„ ìˆ¨ê¹€)")
            else:  # ERROR ì´ìƒ
                self.logger.setLevel(20)  # INFO
                print("ğŸ“ ë¡œê·¸ ë ˆë²¨: INFO (ëª¨ë“  ë©”ì‹œì§€ í‘œì‹œ)")
        
        elif key == ord('w') or key == ord('W'):
            # RGBD ìƒ˜í”Œë§ (í´ë¦­ëœ ìœ„ì¹˜ì˜ ê°’ ì‚¬ìš©)
            if hasattr(self, 'last_clicked_rgbd') and self.last_clicked_rgbd:
                try:
                    sample_data = self.last_clicked_rgbd
                    
                    # RGB í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
                    position = sample_data['position']
                    rgb_values = sample_data['rgb_values']
                    brightness = sample_data['brightness']
                    depth_mm = sample_data.get('depth_mm')
                    
                    # ìƒˆë¡œìš´ RGB í•™ìŠµ ë©”ì„œë“œ ì‚¬ìš©
                    success = self.laser_core.learn_rgb_sample(position, rgb_values, brightness, depth_mm)
                    
                    if success:
                        print(f"[ì„±ê³µ] RGB í•™ìŠµ ì™„ë£Œ: ë°ê¸°={brightness}, RGB={rgb_values}")
                    else:
                        print(f"[ê²½ê³ ] RGB í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                    
                    print(f"âœ… [Wí‚¤ ìƒ˜í”Œë§ ì„±ê³µ] RGBD ë²”ìœ„ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤")
                    print(f"   ìœ„ì¹˜: {sample_data['position']}")
                    print(f"   ë°ê¸°: {sample_data['brightness']}")
                    print(f"   ê¹Šì´: {sample_data['depth_mm']}mm")
                except Exception as e:
                    print(f"âŒ [Wí‚¤ ìƒ˜í”Œë§ ì‹¤íŒ¨] {e}")
            else:
                print("âš ï¸ ë¨¼ì € í™”ë©´ì„ í´ë¦­í•˜ì—¬ ìƒ˜í”Œë§í•  ìœ„ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”")
        
        elif key == ord('r') or key == ord('R'):
            # í†µê³„ ë¦¬ì…‹
            self.stats = {
                'frames_processed': 0,
                'detections': 0,
                'depth_detections': 0,
                'triangulated_points': 0,
                'start_time': time.time(),
                'fps': 0.0
            }
            
        elif key == ord('m') or key == ord('M'):
            # ğŸ¯ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶œë ¥
            print("\n" + "="*60)
            print("ğŸ¯ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
            print("="*60)
            
            perf_stats = self.get_performance_stats()
            
            print("ğŸ“Š FPS ì„±ëŠ¥:")
            current_fps = perf_stats.get('current_fps', 0)
            target_fps = perf_stats.get('target_fps', 50)
            achievement = perf_stats.get('fps_achievement', '0%')
            
            print(f"   í˜„ì¬ FPS: {current_fps}")
            print(f"   ëª©í‘œ FPS: {target_fps}")
            print(f"   ë‹¬ì„±ë¥ : {achievement}")
            
            if current_fps >= 45:
                print("   ìƒíƒœ: âœ… ìš°ìˆ˜ (45+ FPS)")
            elif current_fps >= 30:
                print("   ìƒíƒœ: âš ï¸ ì–‘í˜¸ (30+ FPS)")  
            else:
                print("   ìƒíƒœ: âŒ ê°œì„ í•„ìš” (<30 FPS)")
            
            print("\nğŸ¯ ROI íš¨ìœ¨ì„±:")
            roi_efficiency = perf_stats.get('roi_efficiency', '100%')
            roi_performance = perf_stats.get('roi_performance', 'unknown')
            pixel_reduction = perf_stats.get('pixel_reduction', '0%')
            memory_saved = perf_stats.get('memory_saved', '0%')
            
            print(f"   ROI ë©´ì : {roi_efficiency}")
            print(f"   ì„±ëŠ¥: {roi_performance}")
            print(f"   í”½ì…€ ì ˆì•½: {pixel_reduction}")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved}")
            
            print("\nâš¡ ìµœì í™” íš¨ê³¼:")
            if float(roi_efficiency.replace('%', '')) < 50:
                print("   âœ… ROI ìµœì í™”: ìš°ìˆ˜")
            elif float(roi_efficiency.replace('%', '')) < 80:
                print("   âš ï¸ ROI ìµœì í™”: ì–‘í˜¸")
            else:
                print("   âŒ ROI ìµœì í™”: ê°œì„ í•„ìš”")
            
            print(f"\nğŸš€ ê¶Œì¥ì‚¬í•­:")
            if current_fps < 45:
                print("   â€¢ ROI ì˜ì—­ ë” ì¶•ì†Œ ê¶Œì¥")
                print("   â€¢ ë¶ˆí•„ìš”í•œ ê²€ì¶œ ì˜µì…˜ ë¹„í™œì„±í™”")
            if float(roi_efficiency.replace('%', '')) > 60:
                print("   â€¢ Body Tracking ì •í™•ë„ ê°œì„  í•„ìš”")
                print("   â€¢ ë™ì  ROI ì—…ë°ì´íŠ¸ ì£¼ê¸° ì¡°ì •")
            
            print("="*60)
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ë„ ë¦¬ì…‹
            self.calibration_data = {
                'primary_corners': [],      # 1ë²ˆ ì¹´ë©”ë¼ ì½”ë„ˆ ì¢Œí‘œë“¤ (ê° ì´ë¯¸ì§€ë³„)
                'secondary_corners': [],    # 2ë²ˆ ì¹´ë©”ë¼ ì½”ë„ˆ ì¢Œí‘œë“¤ (ê° ì´ë¯¸ì§€ë³„)
                'object_points': [],        # 3D ì›”ë“œ ì¢Œí‘œ (ê° ì´ë¯¸ì§€ë³„)
                'primary_images': [],       # 1ë²ˆ ì¹´ë©”ë¼ ì›ë³¸ ì´ë¯¸ì§€ë“¤
                'secondary_images': [],     # 2ë²ˆ ì¹´ë©”ë¼ ì›ë³¸ ì´ë¯¸ì§€ë“¤
                'image_count': 0,
                'calibration_complete': False,
                'min_images_required': 15,  # ìµœì†Œ í•„ìš” ì´ë¯¸ì§€ ìˆ˜
                'max_images_allowed': 25,   # ìµœëŒ€ ìˆ˜ì§‘ ì´ë¯¸ì§€ ìˆ˜
                'primary_detected': False,
                'secondary_detected': False,
                'last_capture_time': 0      # ë§ˆì§€ë§‰ ìº¡ì²˜ ì‹œê°„ (ì¤‘ë³µ ë°©ì§€)
            }
            print("[INFO] Statistics and Calibration Data Reset Complete")
        
        elif key == ord('o') or key == ord('O'):
            # [quiet] ROI ì„¤ì • ëª¨ë“œ ì•ˆë‚´
            print("  0: ROI ë¹„í™œì„±í™”")
        
        elif key == ord('1'):
            # ìŠ¤í¬ë¦° ROI ì„¤ì • (í™”ë©´ ì¤‘ì•™ ì˜ì—­)
            h, w = self.current_frame.shape[:2] if self.current_frame is not None else (1080, 1920)
            screen_roi = (w//4, h//4, 3*w//4, 3*h//4)  # ì¤‘ì•™ 50% ì˜ì—­
            self.laser_core.set_roi("screen", screen_roi)
            if hasattr(self.laser_core, 'set_roi_enable'):
                self.laser_core.set_roi_enable(True, user_override=True)
            print(f"[quiet] ìŠ¤í¬ë¦° ROI í™œì„±í™”: ì¤‘ì•™ ì˜ì—­ {screen_roi}")
        
        elif key == ord('2'):
            # ì´êµ¬ ROI ì„¤ì • (í™”ë©´ ìš°ì¸¡ ì˜ì—­)
            h, w = self.current_frame.shape[:2] if self.current_frame is not None else (1080, 1920)
            gun_roi = (2*w//3, h//4, w, 3*h//4)  # ìš°ì¸¡ 1/3 ì˜ì—­
            self.laser_core.set_roi("gun", gun_roi)
            print(f"[quiet] ì´êµ¬ ROI í™œì„±í™”: ìš°ì¸¡ ì˜ì—­ {gun_roi}")
        
        elif key == ord('0'):
            # ROI ë¹„í™œì„±í™”
            if hasattr(self.laser_core, 'set_roi_enable'):
                self.laser_core.set_roi_enable(False, user_override=True)
            else:
                self.laser_core.enable_roi = False
            print("ROI ë¹„í™œì„±í™”(ì‚¬ìš©ì ìš°ì„ ê¶Œ ê³ ì •)")
        
        elif key == ord('=') or key == ord('+'):
            # ë°ê¸° ì„ê³„ê°’ ì¦ê°€
            self.laser_core.brightness_threshold += 10
            print(f"ë°ê¸° ì„ê³„ê°’ ì¦ê°€: {self.laser_core.brightness_threshold}")
        
        elif key == ord('-') or key == ord('_'):
            # ë°ê¸° ì„ê³„ê°’ ê°ì†Œ
            self.laser_core.brightness_threshold = max(10, self.laser_core.brightness_threshold - 10)
            print(f"ë°ê¸° ì„ê³„ê°’ ê°ì†Œ: {self.laser_core.brightness_threshold}")
        
        elif key == ord('['):
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ê°ì†Œ (ë” ê´€ëŒ€í•˜ê²Œ)
            self.laser_core.min_confidence_threshold = max(0.1, self.laser_core.min_confidence_threshold - 0.1)
            print(f"ì‹ ë¢°ë„ ì„ê³„ê°’ ê°ì†Œ: {self.laser_core.min_confidence_threshold:.1f}")
        
        elif key == ord(']'):
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì¦ê°€ (ë” ì—„ê²©í•˜ê²Œ)
            self.laser_core.min_confidence_threshold = min(1.0, self.laser_core.min_confidence_threshold + 0.1)
            print(f"ì‹ ë¢°ë„ ì„ê³„ê°’ ì¦ê°€: {self.laser_core.min_confidence_threshold:.1f}")
        
        elif key == ord(',') or key == ord('<'):
            # ìµœì†Œ ë©´ì  ê°ì†Œ (ë” ì‘ì€ ë ˆì´ì € í¬ì¸íŠ¸ í—ˆìš©)
            self.laser_core.min_laser_area = max(1, self.laser_core.min_laser_area - 1)
            print(f"ìµœì†Œ ë ˆì´ì € ë©´ì  ê°ì†Œ: {self.laser_core.min_laser_area}í”½ì…€")
        
        elif key == ord('.') or key == ord('>'):
            # ìµœì†Œ ë©´ì  ì¦ê°€
            self.laser_core.min_laser_area = min(self.laser_core.max_laser_area - 1, self.laser_core.min_laser_area + 1)
            print(f"ìµœì†Œ ë ˆì´ì € ë©´ì  ì¦ê°€: {self.laser_core.min_laser_area}í”½ì…€")
    
    def run_detection(self):
        """ë©”ì¸ ê²€ì¶œ ë£¨í”„ ì‹¤í–‰"""
        if not self.initialize_system():
            print("[ERROR] System Initialization Failed")
            return
        
        print("\n[INFO] Azure Kinect Laser Detection Started!")
        print("[INFO] Keyboard Commands:")
        print("  D: ë””ë²„ê·¸ ëª¨ë“œ")
        print("  Z: ê¹Šì´ ë§µ í‘œì‹œ")
        print("  T: 3D ì‚¼ê°ì¸¡ëŸ‰ (ë“€ì–¼ ëª¨ë“œ)")
        print("  F: ê¹Šì´ í•„í„°ë§")
        print("  H: Modified CHT í† ê¸€ (Enhanced Mode)")
        print("  E: Enhanced Detector í†µê³„ ì¶œë ¥")
        print("  C: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ (ì²´ìŠ¤ë³´ë“œ ì½”ë„ˆ ê²€ì¶œ)")
        print("  S: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ (C ëª¨ë“œì—ì„œ)")
        print("  P: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ë° ì €ì¥ (C ëª¨ë“œì—ì„œ)")
        print("  L: ë¡œê·¸ ë ˆë²¨ ì¡°ì • (INFO â†’ WARNING â†’ ERROR)")
        print("  W: RGBD ìƒ˜í”Œë§ (í´ë¦­ í›„ Wí‚¤ë¡œ í•™ìŠµ)")
        print("  R: í†µê³„ ë¦¬ì…‹")
        print("  O: ROI ì„¤ì • ëª¨ë“œ")
        print("  1/2/0: ROI ì˜ì—­ ì„ íƒ/ë¹„í™œì„±í™”")
        print("  +/-: ë°ê¸° ì„ê³„ê°’ ì¡°ì •")
        print("  [/]: ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì •")
        print("  ,/.: ìµœì†Œ ë ˆì´ì € ë©´ì  ì¡°ì •")
        print("  ğŸ¯ M: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (FPS, ROI íš¨ìœ¨ì„±)")
        print("  Q: ì¢…ë£Œ")
        print("\n[RGBD DEBUG] ë§ˆìš°ìŠ¤ ì¡°ì‘:")
        print("  ì™¼ìª½ í´ë¦­: RGBD ê°’ í™•ì¸")
        print("  Wí‚¤: í´ë¦­í•œ ìœ„ì¹˜ì˜ RGBD ê°’ ìƒ˜í”Œë§")
        print("  ì˜¤ë¥¸ìª½ í´ë¦­: í˜„ì¬ ê²€ì¶œ ì„¤ì • ì¶œë ¥")
        
        self.is_running = True
        self.mouse_callback_set = False  # ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì • ì—¬ë¶€ í”Œë˜ê·¸
        
        try:
            while self.is_running:
                try:
                    # í”„ë ˆì„ ì²˜ë¦¬ (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)
                    detection_result = self.process_frame()
                    
                    # í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰
                    if detection_result is None:
                        print("[WARN] Frame Processing Failed - Continuing...")
                        time.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°
                        continue
                    
                    # ê¸°ë³¸ ì¹´ë©”ë¼ ì‹œê°í™”
                    display_frame = self.visualize_frame(detection_result)
                    
                    if self.camera_type == "dual_azure_kinect":
                        # ë“€ì–¼ ëª¨ë“œ: ë‘ ê°œì˜ ì°½ í‘œì‹œ (í¬ê¸° ìµœì í™”)
                        # ì£¼ ì¹´ë©”ë¼ ì°½ (í™”ë©´ í¬ê¸°ì˜ 1/2ë¡œ ì¡°ì •, ì‚¬ìš©ìê°€ ì´ë™ ê°€ëŠ¥)
                        display_frame_resized = cv2.resize(display_frame, (960, 540))  # 1920x1080 â†’ 960x540
                        cv2.namedWindow("Camera #1 - Screen Detection (Azure Kinect)", cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)
                        cv2.resizeWindow("Camera #1 - Screen Detection (Azure Kinect)", 960, 540)
                        # ì´ˆê¸° ìœ„ì¹˜ë§Œ ì„¤ì •, ì´í›„ ì‚¬ìš©ìê°€ ììœ ë¡­ê²Œ ì´ë™ ê°€ëŠ¥
                        if not hasattr(self, '_window1_positioned'):
                            cv2.moveWindow("Camera #1 - Screen Detection (Azure Kinect)", 50, 50)
                            self._window1_positioned = True
                        cv2.imshow("Camera #1 - Screen Detection (Azure Kinect)", display_frame_resized)
                        
                        # ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì • (ìœˆë„ìš° ìƒì„± í›„ í•œ ë²ˆë§Œ)
                        if not self.mouse_callback_set:
                            cv2.setMouseCallback("Camera #1 - Screen Detection (Azure Kinect)", self.mouse_callback)
                            self.mouse_callback_set = True
                            print("[DEBUG] ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì • ì™„ë£Œ")
                        
                        # ë³´ì¡° ì¹´ë©”ë¼ ì‹œê°í™” (ë³´ì¡° ì¹´ë©”ë¼ ê²€ì¶œ ê²°ê³¼ í•„ìš”)
                        secondary_result = None
                        if hasattr(self, '_last_secondary_result'):
                            secondary_result = self._last_secondary_result
                        
                        secondary_display = self.visualize_secondary_frame(secondary_result)
                        # ë³´ì¡° ì¹´ë©”ë¼ ì°½ë„ í¬ê¸° ì¡°ì • (ì‚¬ìš©ìê°€ ì´ë™ ê°€ëŠ¥)
                        secondary_display_resized = cv2.resize(secondary_display, (960, 540))
                        cv2.namedWindow("Camera #2 - Gun Detection (Azure Kinect)", cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)
                        cv2.resizeWindow("Camera #2 - Gun Detection (Azure Kinect)", 960, 540)
                        # ì´ˆê¸° ìœ„ì¹˜ë§Œ ì„¤ì •, ì´í›„ ì‚¬ìš©ìê°€ ììœ ë¡­ê²Œ ì´ë™ ê°€ëŠ¥
                        if not hasattr(self, '_window2_positioned'):
                            cv2.moveWindow("Camera #2 - Gun Detection (Azure Kinect)", 1050, 50)
                            self._window2_positioned = True
                        cv2.imshow("Camera #2 - Gun Detection (Azure Kinect)", secondary_display_resized)
                    else:
                        # ë‹¨ì¼ ëª¨ë“œ: í•˜ë‚˜ì˜ ì°½ë§Œ í‘œì‹œ
                        cv2.imshow("Azure Kinect RGB Laser Detection", display_frame)
                        
                        # ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì • (ìœˆë„ìš° ìƒì„± í›„ í•œ ë²ˆë§Œ)
                        if not self.mouse_callback_set:
                            cv2.setMouseCallback("Azure Kinect RGB Laser Detection", self.mouse_callback)
                            self.mouse_callback_set = True
                            print("[DEBUG] ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì • ì™„ë£Œ")
                    
                    # í‚¤ ì…ë ¥ ì²˜ë¦¬
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q') or key == ord('Q'):
                        print("[INFO] Normal Exit with Q Key")
                        break
                    elif key != 255:
                        self.handle_keyboard_input(key)
                        
                except Exception as frame_error:
                    print(f"[WARN] Frame Processing Error (Continuing): {frame_error}")
                    time.sleep(0.1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ì‹œ ëŒ€ê¸°
                    continue
        
        except KeyboardInterrupt:
            print("\n[INFO] Interrupted by User")
        
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("[INFO] System Cleanup...")
        
        self.is_running = False
        
        if self.camera_manager:
            self.camera_manager.cleanup()
        
        cv2.destroyAllWindows()
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        elapsed = time.time() - self.stats['start_time']
        print(f"\n[INFO] Final Statistics:")
        print(f"  ì‹¤í–‰ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"  ì²˜ë¦¬ í”„ë ˆì„: {self.stats['frames_processed']}")
        print(f"  ì´ ê²€ì¶œ: {self.stats['detections']}")
        print(f"  ê¹Šì´ ê¸°ë°˜ ê²€ì¶œ: {self.stats['depth_detections']}")
        if self.camera_type == "dual_azure_kinect":
            print(f"  3D ì‚¼ê°ì¸¡ëŸ‰: {self.stats['triangulated_points']}")
        if elapsed > 0:
            print(f"  í‰ê·  FPS: {self.stats['frames_processed']/elapsed:.1f}")
        
        print("[INFO] Cleanup Complete")
    
    def capture_calibration_image(self):
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ìœ„í•œ ì´ë¯¸ì§€ ìˆ˜ì§‘"""
        try:
            # í˜„ì¬ ì‹œê°„ í™•ì¸ (ì¤‘ë³µ ìº¡ì²˜ ë°©ì§€)
            current_time = time.time()
            if current_time - self.calibration_data['last_capture_time'] < 5.0:  # 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¦ê°€
                remaining = 5.0 - (current_time - self.calibration_data['last_capture_time'])
                print(f"â° ìº¡ì²˜ ê°„ê²©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. {remaining:.1f}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                return
            
            # ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ í™•ì¸
            if self.calibration_data['image_count'] >= self.calibration_data['max_images_allowed']:
                print(f"ğŸ“¸ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜({self.calibration_data['max_images_allowed']})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                return
            
            # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ
            primary_found, primary_corners = self.detect_chessboard_corners(self.current_frame, "primary")
            secondary_found, secondary_corners = self.detect_chessboard_corners(self.secondary_frame, "secondary")
            
            # í”„ë ˆì„ ë™ê¸°í™” ê²€ì¦ (ì„ íƒì )
            if hasattr(self, 'frame_timestamp_diff') and abs(self.frame_timestamp_diff) > 0.1:  # 100ms ì´ìƒ ì°¨ì´
                print(f"âš ï¸ í”„ë ˆì„ ë™ê¸°í™” ê²½ê³ : ì¹´ë©”ë¼ ê°„ ì‹œê°„ì°¨ {self.frame_timestamp_diff*1000:.1f}ms")
            
            # ì–‘ìª½ ì¹´ë©”ë¼ì—ì„œ ëª¨ë‘ ê²€ì¶œë˜ì–´ì•¼ í•¨
            if primary_found and secondary_found:
                # ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ (ì¤‘ìš”!)
                primary_quality_ok, primary_quality_msg = self._validate_image_quality(self.current_frame, primary_corners, "primary")
                secondary_quality_ok, secondary_quality_msg = self._validate_image_quality(self.secondary_frame, secondary_corners, "secondary")
                
                if not primary_quality_ok:
                    print(f"âš ï¸ 1ë²ˆ ì¹´ë©”ë¼ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {primary_quality_msg}")
                    return
                    
                if not secondary_quality_ok:
                    print(f"âš ï¸ 2ë²ˆ ì¹´ë©”ë¼ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {secondary_quality_msg}")
                    return
                
                # 3D ì›”ë“œ ì¢Œí‘œ ìƒì„± (ì²´ìŠ¤ë³´ë“œ íŒ¨í„´)
                object_points = self._generate_object_points()
                
                # ë°ì´í„° ì €ì¥ (íƒ€ì… ì•ˆì „ì„± í™•ë³´)
                if not isinstance(self.calibration_data['primary_corners'], list):
                    self.calibration_data['primary_corners'] = []
                if not isinstance(self.calibration_data['secondary_corners'], list):
                    self.calibration_data['secondary_corners'] = []
                if not isinstance(self.calibration_data['object_points'], list):
                    self.calibration_data['object_points'] = []
                
                self.calibration_data['primary_corners'].append(primary_corners)
                self.calibration_data['secondary_corners'].append(secondary_corners)
                self.calibration_data['object_points'].append(object_points)
                self.calibration_data['primary_images'].append(self.current_frame.copy())
                self.calibration_data['secondary_images'].append(self.secondary_frame.copy())
                self.calibration_data['image_count'] += 1
                self.calibration_data['last_capture_time'] = current_time
                
                print(f"ğŸ“¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì„±ê³µ! ({self.calibration_data['image_count']}/{self.calibration_data['min_images_required']})")
                print(f"âœ… í’ˆì§ˆ: {primary_quality_msg}, {secondary_quality_msg}")
                
                # í¬ì¦ˆ ë‹¤ì–‘ì„± ê°€ì´ë“œ ì œê³µ
                remaining = self.calibration_data['min_images_required'] - self.calibration_data['image_count']
                if remaining > 0:
                    poses = ["ê°€ê¹Œì´", "ë©€ë¦¬", "ì™¼ìª½ ê¸°ìš¸ê¸°", "ì˜¤ë¥¸ìª½ ê¸°ìš¸ê¸°", "ìœ„ìª½", "ì•„ë˜ìª½", "ëŒ€ê°ì„ "]
                    current_pose_idx = (self.calibration_data['image_count'] - 1) % len(poses)
                    next_pose = poses[current_pose_idx] if current_pose_idx < len(poses) else "ë‹¤ì–‘í•œ ê°ë„"
                    print(f"ğŸ’¡ ë‹¤ìŒ í¬ì¦ˆ ì œì•ˆ: ì²´ìŠ¤ë³´ë“œë¥¼ '{next_pose}'ì—ì„œ ì´¬ì˜í•˜ì„¸ìš”")
                
                # ì´ë¯¸ì§€ ì €ì¥ (ì„ íƒì‚¬í•­)
                import os
                calib_dir = "calibration_images"
                if not os.path.exists(calib_dir):
                    os.makedirs(calib_dir)
                
                cv2.imwrite(f"{calib_dir}/primary_{self.calibration_data['image_count']:02d}.jpg", self.current_frame)
                cv2.imwrite(f"{calib_dir}/secondary_{self.calibration_data['image_count']:02d}.jpg", self.secondary_frame)
                
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ ì•ˆë‚´
                if self.calibration_data['image_count'] >= self.calibration_data['min_images_required']:
                    print(f"ğŸ”¬ ì¶©ë¶„í•œ ì´ë¯¸ì§€ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. 'P' í‚¤ë¥¼ ëˆŒëŸ¬ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ê³„ì‚°í•˜ì„¸ìš”.")
            else:
                missing = []
                if not primary_found:
                    missing.append("1ë²ˆ ì¹´ë©”ë¼")
                if not secondary_found:
                    missing.append("2ë²ˆ ì¹´ë©”ë¼")
                print(f"âŒ ì²´ìŠ¤ë³´ë“œ ê²€ì¶œ ì‹¤íŒ¨: {', '.join(missing)}ì—ì„œ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _generate_object_points(self):
        """ì²´ìŠ¤ë³´ë“œ 3D ì›”ë“œ ì¢Œí‘œ ìƒì„±"""
        board_width, board_height = self.calibration_config['chessboard_size']
        square_size = self.calibration_config['square_size']
        
        object_points = np.zeros((board_width * board_height, 3), np.float32)
        object_points[:, :2] = np.mgrid[0:board_width, 0:board_height].T.reshape(-1, 2)
        object_points *= square_size
        
        return object_points
    
    def _validate_image_quality(self, frame: np.ndarray, corners: np.ndarray, camera_name: str) -> Tuple[bool, str]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ (ë¸”ëŸ¬, ê¸°ìš¸ê¸°, ì½”ë„ˆ ì •í™•ë„)"""
        try:
            # 1. ì´ë¯¸ì§€ ì„ ëª…ë„ ê²€ì‚¬ (Laplacian variance)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            min_sharpness = 100.0  # ìµœì†Œ ì„ ëª…ë„ ì„ê³„ê°’
            
            if laplacian_var < min_sharpness:
                return False, f"ì´ë¯¸ì§€ê°€ íë¦¿í•¨ (ì„ ëª…ë„: {laplacian_var:.1f} < {min_sharpness})"
            
            # 2. ì²´ìŠ¤ë³´ë“œ í¬ê¸° ê²€ì¦ (ë„ˆë¬´ ì‘ê±°ë‚˜ í° ê²½ìš° ì œì™¸)
            if len(corners) > 0:
                # ì½”ë„ˆë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                x_coords = corners[:, 0, 0]
                y_coords = corners[:, 0, 1]
                width = np.max(x_coords) - np.min(x_coords)
                height = np.max(y_coords) - np.min(y_coords)
                
                # ì´ë¯¸ì§€ ëŒ€ë¹„ ì²´ìŠ¤ë³´ë“œ í¬ê¸° ë¹„ìœ¨
                img_area = frame.shape[0] * frame.shape[1]
                board_area = width * height
                area_ratio = board_area / img_area
                
                # ì ì ˆí•œ í¬ê¸° ë²”ìœ„ í™•ì¸ (ì´ë¯¸ì§€ì˜ 1-70%) - ì™„í™”ëœ ê¸°ì¤€
                if area_ratio < 0.01:
                    return False, f"ì²´ìŠ¤ë³´ë“œê°€ ë„ˆë¬´ ì‘ìŒ (ë©´ì ë¹„: {area_ratio:.1%})"
                if area_ratio > 0.7:
                    return False, f"ì²´ìŠ¤ë³´ë“œê°€ ë„ˆë¬´ í¼ (ë©´ì ë¹„: {area_ratio:.1%})"
            
            # 3. ì½”ë„ˆ ë¶„í¬ ê· ë“±ì„± ê²€ì¦
            if len(corners) > 4:
                # ì½”ë„ˆë“¤ì´ ì´ë¯¸ì§€ ì „ì²´ì— ê³ ë¥´ê²Œ ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                x_std = np.std(corners[:, 0, 0])
                y_std = np.std(corners[:, 0, 1])
                min_spread = 20.0  # ìµœì†Œ ë¶„ì‚° (ì™„í™”ëœ ê¸°ì¤€)
                
                if x_std < min_spread or y_std < min_spread:
                    return False, f"ì½”ë„ˆ ë¶„í¬ê°€ ë¶ˆê· ë“±í•¨ (Xí¸ì°¨:{x_std:.1f}, Yí¸ì°¨:{y_std:.1f})"
            
            # 4. ì²´ìŠ¤ë³´ë“œ ê¸°ìš¸ê¸° ê²€ì¦ (ì ë‹¹í•œ ê¸°ìš¸ê¸°ëŠ” í—ˆìš©)
            if len(corners) >= 4:
                pattern_size = self.calibration_config['chessboard_size']
                if len(corners) >= pattern_size[0]:
                    # ì²« ë²ˆì§¸ í–‰ì˜ ê¸°ìš¸ê¸°
                    first_row = corners[:pattern_size[0]]
                    dx = first_row[-1, 0, 0] - first_row[0, 0, 0]
                    dy = first_row[-1, 0, 1] - first_row[0, 0, 1]
                    angle = abs(np.degrees(np.arctan2(dy, dx)))
                    
                    # ë„ˆë¬´ ì‹¬í•˜ê²Œ ê¸°ìš¸ì–´ì§„ ê²½ìš°ë§Œ ì œì™¸ (60ë„ ì´ìƒ)
                    if angle > 60 and angle < 120:
                        return False, f"ì²´ìŠ¤ë³´ë“œê°€ ë„ˆë¬´ ê¸°ìš¸ì–´ì§ (ê°ë„: {angle:.1f}Â°)"
            
            return True, f"í’ˆì§ˆ ê²€ì¦ í†µê³¼ (ì„ ëª…ë„: {laplacian_var:.1f})"
            
        except Exception as e:
            return False, f"í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}"
    
    def perform_stereo_calibration(self):
        """ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ë° ì €ì¥"""
        try:
            if self.calibration_data['image_count'] < self.calibration_data['min_images_required']:
                print(f"âŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ìœ„í•œ ìµœì†Œ ì´ë¯¸ì§€ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ({self.calibration_data['image_count']}/{self.calibration_data['min_images_required']})")
                return
            
            print("ğŸ”¬ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ì‹œì‘...")
            
            # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            image_size = (self.current_frame.shape[1], self.current_frame.shape[0])
            
            # ê°œë³„ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
            print("ğŸ“¹ 1ë²ˆ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜...")
            ret1, camera_matrix1, distortion1, rvecs1, tvecs1 = cv2.calibrateCamera(
                self.calibration_data['object_points'],
                self.calibration_data['primary_corners'],
                image_size,
                None, None
            )
            
            print("ğŸ“¹ 2ë²ˆ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜...")
            ret2, camera_matrix2, distortion2, rvecs2, tvecs2 = cv2.calibrateCamera(
                self.calibration_data['object_points'],
                self.calibration_data['secondary_corners'],
                image_size,
                None, None
            )
            
            # ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
            print("ğŸ¯ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜...")
            ret, camera_matrix1, distortion1, camera_matrix2, distortion2, R, T, E, F = cv2.stereoCalibrate(
                self.calibration_data['object_points'],
                self.calibration_data['primary_corners'],
                self.calibration_data['secondary_corners'],
                camera_matrix1, distortion1,
                camera_matrix2, distortion2,
                image_size,
                criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 1e-6),
                flags=cv2.CALIB_FIX_INTRINSIC
            )
            
            # ìŠ¤í…Œë ˆì˜¤ ì •ë¥˜í™”
            print("ğŸ“ ìŠ¤í…Œë ˆì˜¤ ì •ë¥˜í™” ê³„ì‚°...")
            R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
                camera_matrix1, distortion1,
                camera_matrix2, distortion2,
                image_size, R, T,
                flags=cv2.CALIB_ZERO_DISPARITY,
                alpha=0.9
            )
            
            # ì •ë¥˜í™” ë§µ ìƒì„±
            map1x, map1y = cv2.initUndistortRectifyMap(camera_matrix1, distortion1, R1, P1, image_size, cv2.CV_32FC1)
            map2x, map2y = cv2.initUndistortRectifyMap(camera_matrix2, distortion2, R2, P2, image_size, cv2.CV_32FC1)
            
            # ê²°ê³¼ ì €ì¥ (ZED ë²„ì „ê³¼ ìœ ì‚¬í•œ ìƒì„¸ êµ¬ì¡°)
            baseline_mm = float(np.linalg.norm(T))
            convergence_angle = float(np.degrees(np.arccos(np.clip(R[0, 0], -1, 1))))
            
            self.calibration_results = {
                'calibration_info': {
                    'method': 'Azure Kinect Dual Camera Stereo Calibration',
                    'calibration_date': time.strftime('%Y-%m-%dT%H:%M:%S'),
                    'num_images': self.calibration_data['image_count'],
                    'chessboard_size': self.calibration_config['chessboard_size'],
                    'square_size_mm': self.calibration_config['square_size'],
                    'image_resolution': list(image_size),
                    'camera_type': 'Azure Kinect DK'
                },
                'primary_camera_intrinsics': {
                    'camera_matrix': camera_matrix1.tolist(),
                    'distortion': distortion1.tolist(),
                    'rms_error': float(ret1),
                    'fx_fy_ratio': float(camera_matrix1[0, 0] / camera_matrix1[1, 1]),
                    'focal_length_x': float(camera_matrix1[0, 0]),
                    'focal_length_y': float(camera_matrix1[1, 1]),
                    'principal_point_x': float(camera_matrix1[0, 2]),
                    'principal_point_y': float(camera_matrix1[1, 2])
                },
                'secondary_camera_intrinsics': {
                    'camera_matrix': camera_matrix2.tolist(),
                    'distortion': distortion2.tolist(),
                    'rms_error': float(ret2),
                    'fx_fy_ratio': float(camera_matrix2[0, 0] / camera_matrix2[1, 1]),
                    'focal_length_x': float(camera_matrix2[0, 0]),
                    'focal_length_y': float(camera_matrix2[1, 1]),
                    'principal_point_x': float(camera_matrix2[0, 2]),
                    'principal_point_y': float(camera_matrix2[1, 2])
                },
                'stereo_parameters': {
                    'rotation_matrix': R.tolist(),
                    'translation_vector': T.tolist(),
                    'essential_matrix': E.tolist(),
                    'fundamental_matrix': F.tolist(),
                    'stereo_rms_error': float(ret),
                    'baseline_mm': baseline_mm,
                    'rectification_rotation_1': R1.tolist(),
                    'rectification_rotation_2': R2.tolist(),
                    'rectification_projection_1': P1.tolist(),
                    'rectification_projection_2': P2.tolist(),
                    'disparity_to_depth_matrix': Q.tolist()
                },
                'coordinate_system': {
                    'origin': 'Primary camera center',
                    'x_axis': 'Right direction of primary camera',
                    'y_axis': 'Down direction of primary camera', 
                    'z_axis': 'Forward direction of primary camera',
                    'units': 'millimeters',
                    'note': 'Secondary camera position relative to primary camera'
                },
                'quality_metrics': {
                    'overall_stereo_rms_error': float(ret),
                    'primary_camera_rms_error': float(ret1),
                    'secondary_camera_rms_error': float(ret2),
                    'baseline_distance_mm': baseline_mm,
                    'convergence_angle_degrees': convergence_angle,
                    'images_used': self.calibration_data['image_count'],
                    'total_corner_points': self.calibration_data['image_count'] * (self.calibration_config['chessboard_size'][0] * self.calibration_config['chessboard_size'][1])
                }
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥ (í´ë” ìƒì„± í›„ ì €ì¥)
            import json
            import os
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ í´ë” ìƒì„±
            calibration_folder = "calibration_results"
            os.makedirs(calibration_folder, exist_ok=True)
            
            # íŒŒì¼ëª…ê³¼ ì „ì²´ ê²½ë¡œ ìƒì„±
            date_str = self.calibration_results['calibration_info']['calibration_date'].replace(':', '-')
            filename = f"stereo_calibration_{date_str}.json"
            full_path = os.path.join(calibration_folder, filename)
            absolute_path = os.path.abspath(full_path)
            
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(self.calibration_results, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
            print(f"ğŸ“ ì €ì¥ í´ë”: {calibration_folder}/")
            print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {filename}")
            print(f"ğŸ—‚ï¸ ì „ì²´ ê²½ë¡œ: {absolute_path}")
            
            # ìƒì„¸í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ìŠ¤ ì¶œë ¥
            print(f"\nğŸ“Š ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ ë¶„ì„:")
            print(f"  ì „ì²´ ìŠ¤í…Œë ˆì˜¤ RMS ì˜¤ì°¨: {ret:.4f} í”½ì…€")
            print(f"  1ë²ˆ ì¹´ë©”ë¼ RMS ì˜¤ì°¨: {ret1:.4f} í”½ì…€")
            print(f"  2ë²ˆ ì¹´ë©”ë¼ RMS ì˜¤ì°¨: {ret2:.4f} í”½ì…€")
            print(f"  ë² ì´ìŠ¤ë¼ì¸ ê±°ë¦¬: {baseline_mm:.2f} mm")
            print(f"  ìˆ˜ë ´ê°ë„: {convergence_angle:.2f}Â°")
            print(f"  ì‚¬ìš©ëœ ì´ë¯¸ì§€: {self.calibration_data['image_count']}ì¥")
            print(f"  ì´ ì½”ë„ˆ í¬ì¸íŠ¸: {self.calibration_data['image_count'] * (self.calibration_config['chessboard_size'][0] * self.calibration_config['chessboard_size'][1])}ê°œ")
            
            # í’ˆì§ˆ í‰ê°€
            if ret < 0.5:
                print(f"ğŸŸ¢ ìš°ìˆ˜í•œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ!")
            elif ret < 1.0:
                print(f"ğŸŸ¡ ì–‘í˜¸í•œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ")
            elif ret < 2.0:
                print(f"ğŸŸ  í—ˆìš© ê°€ëŠ¥í•œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ")
            else:
                print(f"ğŸ”´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í’ˆì§ˆ ê°œì„  í•„ìš” (ê¶Œì¥: <2.0 í”½ì…€)")
            
            self.calibration_data['calibration_complete'] = True
            
        except Exception as e:
            print(f"âŒ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def mouse_callback(self, event, x, y, flags, param):
        """ë§ˆìš°ìŠ¤ ì½œë°± í•¨ìˆ˜ - RGBD ì‹¤ì‹œê°„ ë””ë²„ê¹…
        - ì»¬ëŸ¬/ê¹Šì´ í•´ìƒë„ ë¶ˆì¼ì¹˜ ì‹œ ì¢Œí‘œ ìŠ¤ì¼€ì¼ ë§¤í•‘ ì ìš©
        - ê¹Šì´ 0(ë¯¸ì¸¡ì •)ì¼ ë•Œ ì£¼ë³€ íƒìƒ‰ìœ¼ë¡œ ë³´ì •
        """
        if self.current_image is None:
            return

        # ì¢Œí‘œ ë° í”„ë ˆì„ í¬ê¸° í™•ë³´
        color_h, color_w = self.current_image.shape[:2]
        depth_available = self.current_depth is not None
        if depth_available:
            depth_h, depth_w = self.current_depth.shape[:2]
        else:
            depth_h = depth_w = 0

        # ì»¬ëŸ¬ ì¢Œí‘œ â†’ ê¹Šì´ ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ (ì •ë ¬ ë¯¸ë³´ì¥ í™˜ê²½ì—ì„œ ê·¼ì‚¬ ë§¤í•‘)
        def color_to_depth_coords(cx: int, cy: int) -> tuple:
            if not depth_available or color_w <= 0 or color_h <= 0:
                return (-1, -1)
            sx = depth_w / float(color_w)
            sy = depth_h / float(color_h)
            dx = int(round(cx * sx))
            dy = int(round(cy * sy))
            # ê²½ê³„ í´ë¦¬í•‘
            dx = max(0, min(depth_w - 1, dx)) if depth_w > 0 else -1
            dy = max(0, min(depth_h - 1, dy)) if depth_h > 0 else -1
            return (dx, dy)

        if event == cv2.EVENT_LBUTTONDOWN:
            # ì™¼ìª½ í´ë¦­: RGBD ê°’ í™•ì¸ ë° ìƒ˜í”Œë§
            if (0 <= y < color_h and 0 <= x < color_w):
                # RGB ê°’ ì¶”ì¶œ
                bgr_pixel = self.current_image[y, x]
                b, g, r = bgr_pixel
                brightness = int(cv2.cvtColor(self.current_image[y:y+1, x:x+1], cv2.COLOR_BGR2GRAY)[0, 0])

                # ê¹Šì´ ê°’ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                depth_mm = None
                if depth_available:
                    dx, dy = color_to_depth_coords(x, y)
                    if dx >= 0 and dy >= 0:
                        raw_depth = self.current_depth[dy, dx]
                        # Azure Kinect ê¹Šì´ ê°’ ìœ íš¨ì„± ê²€ì‚¬
                        if 0 < raw_depth < 65535:
                            depth_mm = float(raw_depth)
                        else:
                            # ì£¼ë³€ í”½ì…€ì—ì„œ ìœ íš¨í•œ ê¹Šì´ ê°’ ì°¾ê¸° (5x5 ì˜ì—­ìœ¼ë¡œ í™•ì¥)
                            found = False
                            for ry in range(-2, 3):
                                for rx in range(-2, 3):
                                    ny, nx = dy + ry, dx + rx
                                    if 0 <= ny < depth_h and 0 <= nx < depth_w:
                                        neighbor_depth = self.current_depth[ny, nx]
                                        if 0 < neighbor_depth < 65535:
                                            depth_mm = float(neighbor_depth)
                                            found = True
                                            break
                                if found:
                                    break
                
                print(f"\n[RGBD DEBUG] í´ë¦­ ìœ„ì¹˜ ({x}, {y}):")
                print(f"  RGB ê°’: R={r}, G={g}, B={b}")
                print(f"  ë°ê¸° ê°’: {brightness}")
                if depth_mm is not None:
                    print(f"  ê¹Šì´ ê°’: {depth_mm}mm ({depth_mm/1000:.2f}m)")
                else:
                    print(f"  ê¹Šì´ ê°’: ì—†ìŒ")
                    print(f"  [ì°¸ê³ ] í™”ë©´/ê´‘íƒ/íˆ¬ëª… í‘œë©´ì€ ê¹Šì´ ì¸¡ì •ì´ ì–´ë ¤ì›€ (ì •ë ¬ ë¯¸ë³´ì • ìƒíƒœì—ì„œëŠ” ê·¼ì‚¬ ì˜¤ì°¨ ê°€ëŠ¥)")
                
                # í˜„ì¬ RGBD ê²€ì¶œ ë²”ìœ„ì™€ ë¹„êµ
                actual_brightness_threshold = self.laser_core.brightness_threshold
                actual_depth_range = self.laser_core.depth_range_mm
                
                brightness_match = brightness >= actual_brightness_threshold
                depth_match = (depth_mm is None or 
                             (actual_depth_range[0] <= depth_mm <= actual_depth_range[1]))
                
                print(f"  ì‹¤ì œ ë°ê¸° ì„ê³„ê°’: {actual_brightness_threshold} ({'í†µê³¼' if brightness_match else 'ì‹¤íŒ¨'})")
                print(f"  ì‹¤ì œ ê¹Šì´ ë²”ìœ„: {actual_depth_range[0]}-{actual_depth_range[1]}mm ({'í†µê³¼' if depth_match else 'ì‹¤íŒ¨'})")
                print(f"  ê²€ì¶œ ê°€ëŠ¥ì„±: {'ê°€ëŠ¥' if brightness_match and depth_match else 'ë¶ˆê°€ëŠ¥'}")
                

                # RGBD ê°’ ì €ì¥ (Wí‚¤ ìƒ˜í”Œë§ìš©)
                self.last_clicked_rgbd = {
                    'position': (x, y),
                    'brightness': brightness,
                    'rgb_values': (r, g, b),
                    'depth_mm': depth_mm if depth_mm is not None else 0,
                    'confidence': 0.8
                }
                print(f"  [íŒ] Wí‚¤ë¥¼ ëˆŒëŸ¬ ì´ RGBD ê°’ì„ ìƒ˜í”Œë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        elif event == cv2.EVENT_RBUTTONDOWN:
            # ì˜¤ë¥¸ìª½ í´ë¦­: ì‹¤ì œ ìƒ˜í”Œë§ëœ RGBD ê²€ì¶œ ì„¤ì • ì¶œë ¥
            print(f"\n[ì‹¤ì œ ê²€ì¶œ ì„¤ì •] í˜„ì¬ Azure Kinect RGBD ê²€ì¶œ ë²”ìœ„:")
            actual_brightness_threshold = self.laser_core.brightness_threshold
            actual_depth_range = self.laser_core.depth_range_mm
            actual_min_area = self.laser_core.min_laser_area
            actual_max_area = self.laser_core.max_laser_area
            
            print(f"  ë°ê¸° ì„ê³„ê°’: {actual_brightness_threshold}")
            print(f"  ê¹Šì´ ë²”ìœ„: {actual_depth_range[0]}-{actual_depth_range[1]}mm")
            print(f"  ë©´ì  ë²”ìœ„: {actual_min_area}-{actual_max_area}í”½ì…€")
            
            # ìƒ˜í”Œë§ëœ ë°ì´í„° ìˆ˜ í™•ì¸
            if hasattr(self.laser_core, 'learned_samples'):
                learned_count = len(self.laser_core.learned_samples)
                print(f"  ìƒ˜í”Œë§ëœ RGB ë°ì´í„° ìˆ˜: {learned_count}ê°œ (RGB í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ)")
                if hasattr(self.laser_core, 'adaptive_brightness_threshold'):
                    print(f"  ì ì‘í˜• ë°ê¸° ì„ê³„ê°’: {self.laser_core.adaptive_brightness_threshold}")
            else:
                print(f"  ìƒ˜í”Œë§ëœ ë°ì´í„° ìˆ˜: 0ê°œ (ì´ˆê¸°í™” í•„ìš”)")
            
            if learned_count == 0:
                print(f"  [ì£¼ì˜] ì•„ì§ ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Wí‚¤ë¡œ ë ˆì´ì €ë¥¼ ìƒ˜í”Œë§í•´ì£¼ì„¸ìš”.")
    
    def _setup_dual_mode_roi(self):
        """ë“€ì–¼ ëª¨ë“œ ì§€ëŠ¥í˜• ROI ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            print("[INFO] ğŸ§  ì§€ëŠ¥í˜• ROI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # HD ëª¨ë“œ ê°•ì œ ì ìš© (ì„±ëŠ¥ ìµœì í™”)
            self.use_4k = False
            width, height = 1920, 1080  # HD í•´ìƒë„ë¡œ ê³ ì •
            print(f"[PERF] ğŸš€ HD ëª¨ë“œ ì ìš©: {width}x{height} @ 30 FPS â†’ 50+ FPS ëª©í‘œ")
            
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            self._apply_performance_optimizations()
            
            # Screen ROI/Polygon: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¸°ì¤€(1920x1080)ì—ì„œ ê³„ì‚° í›„ ìº¡ì²˜ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼
            calib_base = (1920, 1080)
            calib_roi = self._calculate_screen_roi_from_calibration(calib_base[0], calib_base[1])
            camera_info = self.camera_manager.get_camera_info()
            cap_w, cap_h = camera_info.get('resolution', (calib_base[0], calib_base[1]))
            screen_roi = self._scale_roi_to_capture(calib_roi, calib_base, (cap_w, cap_h))
            # í´ë¦¬ê³¤ ê³„ì‚° ë° ì ìš©
            try:
                polygon = self._calculate_screen_polygon_from_calibration(calib_base[0], calib_base[1])
                if polygon and len(polygon) >= 3:
                    sx = cap_w / float(calib_base[0])
                    sy = cap_h / float(calib_base[1])
                    scaled_poly = [(int(x * sx), int(y * sy)) for (x, y) in polygon]
                    if hasattr(self.laser_core, 'set_screen_polygon'):
                        self.laser_core.set_screen_polygon(scaled_poly)
                        print(f"[ROI] ìŠ¤í¬ë¦° í´ë¦¬ê³¤ ì ìš©: {scaled_poly}")
            except Exception as e:
                print(f"[WARN] ìŠ¤í¬ë¦° í´ë¦¬ê³¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # Muzzle ROI: Body Tracking ê¸°ë°˜ ë™ì  ROI (ì´ˆê¸°ê°’ì€ ìƒì²´ ì¤‘ì‹¬)
            initial_muzzle_roi = self._calculate_initial_muzzle_roi(cap_w, cap_h)
            
            # Laser Coreì— ROI ì„¤ì •
            self.laser_core.set_roi("screen", screen_roi)
            self.laser_core.set_roi("gun", initial_muzzle_roi)
            
            # âœ… ì§€ëŠ¥í˜• ROI í™œì„±í™” (ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ ìš°ì„  ì¡´ì¤‘)
            if hasattr(self.laser_core, 'set_roi_enable'):
                self.laser_core.set_roi_enable(True, user_override=False)
            else:
                self.laser_core.enable_roi = True
            print("[SUCCESS] ì§€ëŠ¥í˜• ROI ì‹œìŠ¤í…œ í™œì„±í™”")
            
            # ROI ì—…ë°ì´íŠ¸ ì¹´ìš´í„° ì´ˆê¸°í™”
            self.roi_update_counter = 0
            self.roi_update_interval = 5  # 5í”„ë ˆì„ë§ˆë‹¤ ROI ì—…ë°ì´íŠ¸
            
            print(f"[ROI] ìŠ¤í¬ë¦° ì˜ì—­: {screen_roi}")
            print(f"[ROI] ì´êµ¬ ì˜ì—­ (ì´ˆê¸°): {initial_muzzle_roi}")
            print("[INFO] ğŸ¯ ì§€ëŠ¥í˜• ROI ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ - 50+ FPS ëª©í‘œ")
            
        except Exception as e:
            print(f"[ERROR] ì§€ëŠ¥í˜• ROI ì„¤ì • ì‹¤íŒ¨: {e}")
            # Fallback: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘
            if hasattr(self.laser_core, 'set_roi_enable'):
                self.laser_core.set_roi_enable(False, user_override=False)
            else:
                self.laser_core.enable_roi = False
            print("[FALLBACK] ROI ë¹„í™œì„±í™”ë¡œ ëŒ€ì²´")
    
    def _calculate_screen_roi_from_calibration(self, width: int, height: int) -> Tuple[int, int, int, int]:
        """Calibration ë°ì´í„° ê¸°ë°˜ ì •í™•í•œ ìŠ¤í¬ë¦° ROI ê³„ì‚°"""
        try:
            # calibration íŒŒì¼ì—ì„œ screen_plane ë°ì´í„° ë¡œë“œ
            import json
            with open('azure_kinect_3d_calibration.json', 'r') as f:
                calibration = json.load(f)
            
            # ìŠ¤í¬ë¦° 3D ì½”ë„ˆ ì¢Œí‘œë“¤
            corners_3d = calibration['screen_plane']['corners_3d']
            screen_intrinsics = calibration['screen_camera_intrinsics']
            
            # 3D â†’ 2D íˆ¬ì˜
            corners_2d = []
            fx, fy = screen_intrinsics['fx'], screen_intrinsics['fy']  
            cx, cy = screen_intrinsics['cx'], screen_intrinsics['cy']
            
            for x3d, y3d, z3d in corners_3d:
                if z3d > 0:  # ì¹´ë©”ë¼ ì•ìª½ì— ìˆëŠ” ì ë§Œ
                    x2d = int(fx * x3d / z3d + cx)
                    y2d = int(fy * y3d / z3d + cy)
                    # í™”ë©´ ë²”ìœ„ ë‚´ í´ë¦¬í•‘
                    x2d = max(0, min(width-1, x2d))
                    y2d = max(0, min(height-1, y2d))
                    corners_2d.append((x2d, y2d))
            
            if len(corners_2d) >= 3:
                # Bounding Rectangle ê³„ì‚°
                x_coords = [p[0] for p in corners_2d]
                y_coords = [p[1] for p in corners_2d]
                
                x1, x2 = min(x_coords), max(x_coords)
                y1, y2 = min(y_coords), max(y_coords)
                
                # 10% ë§ˆì§„ ì¶”ê°€ (íˆ¬ì˜ ì˜¤ì°¨ ë³´ì •)
                margin_x = int((x2 - x1) * 0.1)
                margin_y = int((y2 - y1) * 0.1)
                
                roi = (
                    max(0, x1 - margin_x),
                    max(0, y1 - margin_y), 
                    min(width, x2 + margin_x),
                    min(height, y2 + margin_y)
                )
                
                print(f"[CALIBRATION] ìŠ¤í¬ë¦° ROI ê³„ì‚° ì„±ê³µ: {roi}")
                return roi
            
        except Exception as e:
            print(f"[WARN] Calibration ê¸°ë°˜ ROI ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # Fallback: ì¤‘ì•™ 80% ì˜ì—­
        margin_x, margin_y = int(width * 0.1), int(height * 0.1)
        fallback_roi = (margin_x, margin_y, width - margin_x, height - margin_y)
        print(f"[FALLBACK] ê¸°ë³¸ ìŠ¤í¬ë¦° ROI ì‚¬ìš©: {fallback_roi}")
        return fallback_roi

    def _calculate_screen_polygon_from_calibration(self, width: int, height: int) -> Optional[List[Tuple[int,int]]]:
        """Calibration ë°ì´í„° ê¸°ë°˜ ìŠ¤í¬ë¦° 2D í´ë¦¬ê³¤(í”½ì…€) ê³„ì‚°
        - 3D ìŠ¤í¬ë¦° ì½”ë„ˆë¥¼ ì¹´ë©”ë¼1(ìŠ¤í¬ë¦° ì¹´ë©”ë¼) ë‚´ì°¸ìˆ˜ë¡œ íˆ¬ì˜í•˜ì—¬ 2D í´ë¦¬ê³¤ì„ ë§Œë“ ë‹¤
        """
        try:
            import json
            with open('azure_kinect_3d_calibration.json', 'r') as f:
                calibration = json.load(f)

            corners_3d = calibration['screen_plane']['corners_3d']
            intr = calibration['screen_camera_intrinsics']
            fx, fy, cx, cy = intr['fx'], intr['fy'], intr['cx'], intr['cy']

            pts: List[Tuple[int,int]] = []
            for x3d, y3d, z3d in corners_3d:
                if z3d > 0:
                    u = int(fx * x3d / z3d + cx)
                    v = int(fy * y3d / z3d + cy)
                    u = max(0, min(width - 1, u))
                    v = max(0, min(height - 1, v))
                    pts.append((u, v))

            if len(pts) >= 3:
                # ì‹œê³„/ë°˜ì‹œê³„ ì •ë ¬ ë³´ì¥(ì»¨ë²¡ìŠ¤ í— í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨ ì •ë ¬)
                import numpy as np
                pts_np = np.array(pts, dtype=np.int32)
                hull = cv2.convexHull(pts_np)
                hull_list = [(int(p[0][0]), int(p[0][1])) for p in hull]
                return hull_list
        except Exception as e:
            print(f"[WARN] ìŠ¤í¬ë¦° í´ë¦¬ê³¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return None

    def _scale_roi_to_capture(self, roi: Tuple[int, int, int, int], from_size: Tuple[int, int], to_size: Tuple[int, int]) -> Tuple[int, int, int, int]:
        """ROIë¥¼ ê¸°ì¤€ í•´ìƒë„(from_size)ì—ì„œ ì‹¤ì œ ìº¡ì²˜ í•´ìƒë„(to_size)ë¡œ ìŠ¤ì¼€ì¼ ë³€í™˜
        
        Args:
            roi: (x1, y1, x2, y2) in from_size pixels
            from_size: (width_from, height_from)
            to_size: (width_to, height_to)
        Returns:
            ìŠ¤ì¼€ì¼ëœ ROI (ì •ìˆ˜, í™”ë©´ ê²½ê³„ ë‚´ í´ë¦¬í•‘)
        """
        try:
            fx = float(to_size[0]) / max(1, from_size[0])
            fy = float(to_size[1]) / max(1, from_size[1])
            x1, y1, x2, y2 = roi
            sx1 = int(round(x1 * fx))
            sy1 = int(round(y1 * fy))
            sx2 = int(round(x2 * fx))
            sy2 = int(round(y2 * fy))
            # ê²½ê³„ í´ë¦¬í•‘
            sx1 = max(0, min(to_size[0]-1, sx1))
            sy1 = max(0, min(to_size[1]-1, sy1))
            sx2 = max(0, min(to_size[0], sx2))
            sy2 = max(0, min(to_size[1], sy2))
            return (sx1, sy1, sx2, sy2)
        except Exception:
            return roi
    
    def _calculate_initial_muzzle_roi(self, width: int, height: int) -> Tuple[int, int, int, int]:
        """ì´ˆê¸° ì´êµ¬ ROI ì„¤ì • (Body Tracking í™œì„±í™” ì „ê¹Œì§€ ì‚¬ìš©)"""
        # ìƒì²´ ì¤‘ì‹¬ ì˜ì—­ (ì´êµ¬ê°€ ì£¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìœ„ì¹˜)
        center_x, center_y = width // 2, height // 3  # í™”ë©´ ì¤‘ì•™ ìƒë‹¨
        roi_size = 400  # 400x400 í”½ì…€ ì˜ì—­
        
        x1 = max(0, center_x - roi_size // 2)
        y1 = max(0, center_y - roi_size // 2)
        x2 = min(width, center_x + roi_size // 2)
        y2 = min(height, center_y + roi_size // 2)
        
        roi = (x1, y1, x2, y2)
        print(f"[INITIAL] ì´êµ¬ ROI (Body Tracking ëŒ€ê¸°): {roi}")
        return roi
    
    def _update_dynamic_muzzle_roi(self, bt_result, width: int, height: int) -> Tuple[int, int, int, int]:
        """Body Tracking ê²°ê³¼ ê¸°ë°˜ ë™ì  ì´êµ¬ ROI ì—…ë°ì´íŠ¸"""
        try:
            if (bt_result and 
                hasattr(bt_result, 'wrist_2d') and bt_result.wrist_2d and
                hasattr(bt_result, 'index_tip_2d') and bt_result.index_tip_2d):
                
                wrist_x, wrist_y = bt_result.wrist_2d
                index_x, index_y = bt_result.index_tip_2d
                
                # ì†ëª© â†’ ì†ê°€ë½ ë²¡í„° ê³„ì‚°
                vec_x = index_x - wrist_x
                vec_y = index_y - wrist_y
                vec_len = (vec_x**2 + vec_y**2)**0.5
                
                if vec_len > 10:  # ìµœì†Œ ë²¡í„° ê¸¸ì´ ê²€ì¦
                    # ë²¡í„° ì •ê·œí™”
                    vec_x /= vec_len
                    vec_y /= vec_len
                    
                    # ì´ê¸° ê¸¸ì´ ê°€ì • (600mm â†’ í”½ì…€ ë³€í™˜)
                    gun_length_px = 150  # ëŒ€ëµì ì¸ í”½ì…€ ê¸¸ì´
                    
                    # ì´êµ¬ ìœ„ì¹˜ ì˜ˆì¸¡ (ì†ê°€ë½ â†’ ì´êµ¬ ë°©í–¥ìœ¼ë¡œ ì—°ì¥)
                    muzzle_x = int(index_x + vec_x * gun_length_px)
                    muzzle_y = int(index_y + vec_y * gun_length_px)
                    
                    # ROI ì˜ì—­ ì„¤ì • (300x300 í”½ì…€, ì¶©ë¶„í•œ ì—¬ìœ )
                    roi_size = 300
                    x1 = max(0, muzzle_x - roi_size // 2)
                    y1 = max(0, muzzle_y - roi_size // 2)
                    x2 = min(width, muzzle_x + roi_size // 2)
                    y2 = min(height, muzzle_y + roi_size // 2)
                    
                    roi = (x1, y1, x2, y2)
                    print(f"[DYNAMIC] ì´êµ¬ ROI ì—…ë°ì´íŠ¸: {roi} (ì˜ˆì¸¡ ìœ„ì¹˜: {muzzle_x},{muzzle_y})")
                    return roi
            
        except Exception as e:
            print(f"[WARN] ë™ì  ROI ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # Fallback: í˜„ì¬ ROI ìœ ì§€ ë˜ëŠ” ì´ˆê¸° ROI
        return self._calculate_initial_muzzle_roi(width, height)
    
    def _update_intelligent_roi(self):
        """ì§€ëŠ¥í˜• ROI ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ (5í”„ë ˆì„ë§ˆë‹¤ í˜¸ì¶œ)"""
        try:
            # ì‹¤ì œ ìº¡ì²˜ í•´ìƒë„ë¥¼ ì‚¬ìš© (ì„±ëŠ¥ ë° ì¢Œí‘œ ì¼ì¹˜)
            camera_info = self.camera_manager.get_camera_info()
            cap_w, cap_h = camera_info.get('resolution', (1920, 1080))
            width, height = int(cap_w), int(cap_h)
            
            # Body Tracking ê²°ê³¼ í™•ì¸
            bt_result = None
            if hasattr(self, 'bt_worker') and self.bt_worker:
                bt_result = self.bt_worker.get_latest_result()
            
            # ë™ì  ì´êµ¬ ROI ì—…ë°ì´íŠ¸ (ì´ë¯¸ cap_w, cap_h ê¸°ë°˜)
            if bt_result:
                new_muzzle_roi = self._update_dynamic_muzzle_roi(bt_result, width, height)
                self.laser_core.set_roi("gun", new_muzzle_roi)
                # [quiet] ì´êµ¬ ROI ë™ì  ì—…ë°ì´íŠ¸
            else:
                # Body Tracking ì‹¤íŒ¨ ì‹œ ë” ë„“ì€ ì˜ì—­ìœ¼ë¡œ í™•ì¥
                fallback_roi = self._calculate_fallback_muzzle_roi(width, height)
                self.laser_core.set_roi("gun", fallback_roi)
                # [quiet] ì´êµ¬ ROI í´ë°± ì ìš©
                
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if hasattr(self, 'stats'):
                roi_area_ratio = self._calculate_roi_efficiency()
                if roi_area_ratio < 0.5:  # ROIê°€ ì „ì²´ì˜ 50% ë¯¸ë§Œì´ë©´ íš¨ìœ¨ì 
                    self.stats['roi_performance'] = 'excellent'
                elif roi_area_ratio < 0.8:
                    self.stats['roi_performance'] = 'good'
                else:
                    self.stats['roi_performance'] = 'poor'
                    
        except Exception as e:
            print(f"[ERROR] ROI ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _calculate_fallback_muzzle_roi(self, width: int, height: int) -> Tuple[int, int, int, int]:
        """Body Tracking ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì´êµ¬ ROI"""
        # ìƒì²´ ì¤‘ì‹¬ìœ¼ë¡œ ë” ë„“ì€ ì˜ì—­ ì„¤ì •
        center_x, center_y = width // 2, height // 3
        roi_size = 600  # ë” í° ì˜ì—­
        
        x1 = max(0, center_x - roi_size // 2)
        y1 = max(0, center_y - roi_size // 2)
        x2 = min(width, center_x + roi_size // 2)
        y2 = min(height, center_y + roi_size // 2)
        
        return (x1, y1, x2, y2)
    
    def _calculate_roi_efficiency(self) -> float:
        """í˜„ì¬ ROI íš¨ìœ¨ì„± ê³„ì‚° (ì „ì²´ ëŒ€ë¹„ ROI ë©´ì  ë¹„ìœ¨)"""
        try:
            total_pixels = 1920 * 1080  # HD í•´ìƒë„
            
            # ìŠ¤í¬ë¦° ROI ë©´ì 
            screen_roi = getattr(self.laser_core, '_roi_regions', {}).get('screen')
            screen_area = 0
            if screen_roi:
                x1, y1, x2, y2 = screen_roi
                screen_area = (x2 - x1) * (y2 - y1)
            
            # ì´êµ¬ ROI ë©´ì   
            gun_roi = getattr(self.laser_core, '_roi_regions', {}).get('gun')
            gun_area = 0
            if gun_roi:
                x1, y1, x2, y2 = gun_roi
                gun_area = (x2 - x1) * (y2 - y1)
            
            # ì „ì²´ ROI ë©´ì  ë¹„ìœ¨
            total_roi_area = screen_area + gun_area
            efficiency = total_roi_area / total_pixels if total_pixels > 0 else 1.0
            
            return efficiency
            
        except Exception:
            return 1.0  # ê³„ì‚° ì‹¤íŒ¨ ì‹œ ìµœì•…ê°’ ë°˜í™˜
    
    def _apply_performance_optimizations(self):
        """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
        try:
            print("[PERF] âš¡ ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
            
            # 1. Azure Kinect ì„¤ì • ìµœì í™”
            if hasattr(self, 'azure_kinect_config'):
                # ê¹Šì´ í•„í„°ë§ í™œì„±í™” (ë¶ˆí•„ìš”í•œ í”½ì…€ ì œê±°)
                self.azure_kinect_config['use_depth_filtering'] = True
                self.azure_kinect_config['depth_filter_range'] = (500, 8000)  # 0.5m ~ 8m
                
                # ê¹Šì´ í•´ìƒë„ ìµœì í™” (NFOV 2x2 BINNED for dual stability)
                self.azure_kinect_config['depth_mode'] = 'NFOV_2X2BINNED'
                
                # í”„ë ˆì„ ìŠ¤í‚µ ì„¤ì • (ì•ˆì •ì„± í–¥ìƒ)
                self.azure_kinect_config['frame_skip_threshold'] = 2
            
            # 2. ë ˆì´ì € ê²€ì¶œ ìµœì í™”
            if hasattr(self, 'laser_core'):
                # OpenCV ìµœì í™” í”Œë˜ê·¸ ì„¤ì •
                import cv2
                cv2.setUseOptimized(True)
                cv2.setNumThreads(4)  # ë©€í‹°ìŠ¤ë ˆë“œ í™œìš©
                
                # ë¶ˆí•„ìš”í•œ ê²€ì¶œ ë‹¨ê³„ ë¹„í™œì„±í™”
                if hasattr(self.laser_core, 'config'):
                    self.laser_core.config['enable_morphology'] = False  # í˜•íƒœí•™ì  ì—°ì‚° ë¹„í™œì„±í™”
                    self.laser_core.config['enable_contour_filter'] = True  # í•„ìˆ˜ í•„í„°ë§Œ ìœ ì§€
            
            # 3. í”„ë ˆì„ ì²˜ë¦¬ ìµœì í™”
            self.roi_update_interval = 10  # ROI ì—…ë°ì´íŠ¸ ì£¼ê¸° ì—°ì¥ (5â†’10í”„ë ˆì„)
            
            # 4. í†µê³„ ìˆ˜ì§‘ ìµœì í™”
            if hasattr(self, 'stats'):
                self.stats['performance_optimizations_applied'] = True
                self.stats['target_fps'] = 50
                self.stats['roi_efficiency_target'] = 0.5  # 50% ì´í•˜ ROI ë©´ì  ëª©í‘œ
            
            # í•´ìƒë„ ì •ë³´ í‘œì‹œë¥¼ ì‹¤ì œ ìº¡ì²˜ í•´ìƒë„ ê¸°ì¤€ìœ¼ë¡œ ì¶œë ¥
            try:
                cam_info = self.camera_manager.get_camera_info()
                cap_w, cap_h = cam_info.get('resolution', (1920, 1080))
            except Exception:
                cap_w, cap_h = (1920, 1080)

            print("[PERF] âœ… ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ:")
            print(f"   â€¢ ìº¡ì²˜ í•´ìƒë„ ({cap_w}x{cap_h})")
            print("   â€¢ ì§€ëŠ¥í˜• ROI í™œì„±í™”")  
            print("   â€¢ ê¹Šì´ í•„í„°ë§ í™œì„±í™”")
            print("   â€¢ OpenCV ë©€í‹°ìŠ¤ë ˆë“œ")
            print("   â€¢ ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°")
            print("   â€¢ ëª©í‘œ: 50+ FPS")
            
        except Exception as e:
            print(f"[ERROR] ì„±ëŠ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_performance_stats(self) -> dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        try:
            stats = {}
            
            # ê¸°ë³¸ FPS ê³„ì‚°
            if hasattr(self, 'stats') and 'start_time' in self.stats:
                runtime = time.time() - self.stats['start_time']
                if runtime > 0 and 'frames_processed' in self.stats:
                    current_fps = self.stats['frames_processed'] / runtime
                    stats['current_fps'] = round(current_fps, 1)
                    stats['target_fps'] = 50
                    stats['fps_achievement'] = f"{(current_fps/50)*100:.1f}%" if current_fps > 0 else "0%"
            
            # ROI íš¨ìœ¨ì„±
            roi_efficiency = self._calculate_roi_efficiency()
            stats['roi_efficiency'] = f"{roi_efficiency*100:.1f}%"
            stats['roi_performance'] = self.stats.get('roi_performance', 'unknown') if hasattr(self, 'stats') else 'unknown'
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ëŒ€ëµì )
            pixel_reduction = (1 - roi_efficiency) * 100
            stats['pixel_reduction'] = f"{pixel_reduction:.1f}%"
            stats['memory_saved'] = f"{pixel_reduction * 0.8:.1f}%"  # ëŒ€ëµì  ë©”ëª¨ë¦¬ ì ˆì•½
            
            return stats
            
        except Exception as e:
            return {'error': str(e)}

# ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
class MainController(MainControllerAzureKinect):
    """ê¸°ì¡´ MainControllerì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    
    def __init__(self, camera_id: int = 0):
        # ê¸°ì¡´ ë°©ì‹: camera_idë¡œ ì›¹ìº  ì‚¬ìš©
        super().__init__(camera_type="webcam", device_id=camera_id)